{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic | The Power of Sklearn\n",
    "> Sklearn is the most powerful package in all ML libraries but, do you really use it to the fullest?! In this notebook, we will try to investigate deep concepts such as ColumnTransformers, Piplines, and much more.\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Ahmed Abulkhair\n",
    "- categories: [Machine Learning, Sklearn, Python]\n",
    "- image: images/titanic.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "*Neither Titanic dataset nor sklearn a new thing for any data scientist but there are some important features in scikit-learn that will make any model preprocessing and tuning easier, to be specific this notebook will cover the following concepts:*\n",
    "\n",
    ">- ColumnTransformer\n",
    ">- Pipeline\n",
    ">- SimpleImputer\n",
    ">- StandardScalar\n",
    ">- OneHotEncoder\n",
    ">- OrdinalEncoder\n",
    ">- GridSearch\n",
    "\n",
    "\n",
    "Note, this tutorial is a solution to the famous kaggle competition [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mounting Filesystem**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/gender_submission.csv\n",
      "/kaggle/input/titanic/test.csv\n",
      "/kaggle/input/titanic/train.csv\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas for data reading and writing\n",
    "import pandas as pd\n",
    "# Numpy for Numerical operations\n",
    "import numpy as np\n",
    "# Import ColumnTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# Import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Import SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Import StandardScaler, OneHotEncodr and OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "# Import Random Forest for Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import GridSearch\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reading Data**\n",
    "In the following cells, we will read the train and test data and check for NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Read the train data\n",
    "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "# See some info\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obvious that we had to deal with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            332 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           417 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Splitting Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into predictors and target\n",
    "X_train = train_data.drop(['Survived', 'Name'], axis = 1)\n",
    "X_test = test_data.drop(['Name'], axis = 1)\n",
    "y_train = train_data['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Continuous and Numerical features handling**\n",
    "*It's clear that we have some numerical features that have some missing values to be imputed and they have to be of the same scale also.*\n",
    "\n",
    "*In the following cell, we will handle the numerical features separtely i.e \"Age\" and \"Fare\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will create a pipline for the numeric features\n",
    "# Difine a list with the numeric features\n",
    "numeric_features = ['Age', 'Fare']\n",
    "# Define a pipeline for numer\"ic features\n",
    "numeric_features_pipeline = Pipeline(steps= [\n",
    "    ('imputer', SimpleImputer(strategy = 'median')), # Impute with median value for missing\n",
    "    ('scaler', StandardScaler())                     # Conduct a scaling step\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Categorical features handling**\n",
    "*It's clear that we have some categorical features that have some missing values to be imputed and they have to be encoded using one hot encoding.*\n",
    "\n",
    "*In the following cell, we will handle the categorical features separtely i.e \"Embarked\" and \"Sex\"*\n",
    "\n",
    "*Note: I choose simple imputer for the missing cells to impute with 'missing' word. My aim was to gather all missing cells in one category for further encoding.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will create a pipline for the categorical features\n",
    "# Difine a list with the categorical features\n",
    "categorical_features = ['Embarked', 'Sex']\n",
    "# Define a pipeline for categorical features\n",
    "categorical_features_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = 'missing')), # Impute with the word 'missing' for missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))     # Convert all categorical variables to one hot encoding\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ordinal features handling**\n",
    "*Passenger class or 'Pclass' for short is an ordinal feature that must be handled keeping in mind that class 3 is much higher than 2 and so on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will create a pipline for the ordinal features\n",
    "# Define a list with the ordinal features\n",
    "ordinal_features = ['Pclass']\n",
    "# Define a pipline for ordinal features \n",
    "ordinal_features_pipeline = Pipeline(steps=[\n",
    "    ('ordinal', OrdinalEncoder(categories= [[1, 2, 3]]))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Construct a comprehended preprocessor**\n",
    "*Now, we will create a preprocessor that can handle all columns in our dataset using ColumnTransformer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will create a transformer to handle all columns\n",
    "preprocessor = ColumnTransformer(transformers= [\n",
    "    # transformer with name 'num' that will apply\n",
    "    # 'numeric_features_pipeline' to numeric_features\n",
    "    ('num', numeric_features_pipeline, numeric_features),\n",
    "    # transformer with name 'cat' that will apply \n",
    "    # 'categorical_features_pipeline' to categorical_features\n",
    "    ('cat', categorical_features_pipeline, categorical_features),\n",
    "    # transformer with name 'ord' that will apply \n",
    "    # 'ordinal_features_pipeline' to ordinal_features\n",
    "    ('ord', ordinal_features_pipeline, ordinal_features) \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prediction Pipeline**\n",
    "*Now, we will create a full prediction pipeline that uses our preprocessor and then transfer it to our classifier of choice 'Random Forest'*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will create a full prediction pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                     ('classifier', RandomForestClassifier(n_estimators = 120, max_leaf_nodes = 100))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline Training**\n",
    "*Let's train our pipeline now*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='median',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler(copy=True,\n",
       "                                                                                  with_mean...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=100, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=120, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's fit our classifier\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline Tuning**\n",
    "*The question now, can we push it a little bit further? i.e. can we tune every single part or our Pipeline?*\n",
    "\n",
    "*Here, I will use GridSearch to decide three things:*\n",
    ">- Simple Imputer strategy : mean or median\n",
    ">- n_estimators of Random Forest\n",
    ">- max leaf nodes of Random Forest\n",
    "\n",
    "*Note, you can access any parameter from the outer level to the next adjacent inner one*\n",
    "\n",
    "*For Example: to access the strategy of the Simple Imputer you can do the following*\n",
    "preprocessor__num__imputer__strategy\n",
    "\n",
    "*Let's see this into action*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best random forest from grid search: 0.944\n",
      "The best parameters of Simple Imputer and C are:\n",
      "{'classifier__max_leaf_nodes': 100, 'classifier__n_estimators': 150, 'preprocessor__num__imputer__strategy': 'median'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__n_estimators': [100, 120, 150, 170, 200],\n",
    "    'classifier__max_leaf_nodes' : [100, 120, 150, 170, 200]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print((\"best random forest from grid search: %.3f\"\n",
    "       % grid_search.score(X_train, y_train)))\n",
    "print('The best parameters of Simple Imputer and C are:')\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Predictions**\n",
    "*Let's generate predictions now using our grid search model and submit the results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission CSV has been saved!\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predictions = grid_search.predict(X_test)\n",
    "# Generate results dataframe\n",
    "results_df = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "# Save to csv file\n",
    "results_df.to_csv('submission.csv', index = False)\n",
    "print('Submission CSV has been saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
