<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>An Example Markdown Post | Analixa</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="An Example Markdown Post" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A minimal example of using markdown with fastpages." />
<meta property="og:description" content="A minimal example of using markdown with fastpages." />
<link rel="canonical" href="https://aaabulkhair.github.io/Analixa/markdown/2022/03/08/GPT-3,-The-Model-Simply-Knows.html" />
<meta property="og:url" content="https://aaabulkhair.github.io/Analixa/markdown/2022/03/08/GPT-3,-The-Model-Simply-Knows.html" />
<meta property="og:site_name" content="Analixa" />
<meta property="og:image" content="https://aaabulkhair.github.io/Analixa/images/GPT-3-preview.gif" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-08T00:00:00-06:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://aaabulkhair.github.io/Analixa/images/GPT-3-preview.gif" />
<meta property="twitter:title" content="An Example Markdown Post" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-03-08T00:00:00-06:00","datePublished":"2022-03-08T00:00:00-06:00","description":"A minimal example of using markdown with fastpages.","headline":"An Example Markdown Post","image":"https://aaabulkhair.github.io/Analixa/images/GPT-3-preview.gif","mainEntityOfPage":{"@type":"WebPage","@id":"https://aaabulkhair.github.io/Analixa/markdown/2022/03/08/GPT-3,-The-Model-Simply-Knows.html"},"url":"https://aaabulkhair.github.io/Analixa/markdown/2022/03/08/GPT-3,-The-Model-Simply-Knows.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Analixa/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://aaabulkhair.github.io/Analixa/feed.xml" title="Analixa" /><link rel="shortcut icon" type="image/x-icon" href="/Analixa/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Analixa/">Analixa</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Analixa/about/">About Me</a><a class="page-link" href="/Analixa/search/">Search</a><a class="page-link" href="/Analixa/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">An Example Markdown Post</h1><p class="page-description">A minimal example of using markdown with fastpages.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-03-08T00:00:00-06:00" itemprop="datePublished">
        Mar 8, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Analixa/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#gpt-3-the-model-simply-knows">GPT-3, The Model Simply Knows!</a>
<ul>
<li class="toc-entry toc-h2"><a href="#attention-is-all-you-need">Attention is All You Need</a></li>
<li class="toc-entry toc-h2"><a href="#bert">BERT</a></li>
<li class="toc-entry toc-h2"><a href="#gpt-3">GPT-3</a>
<ul>
<li class="toc-entry toc-h3"><a href="#training-datasets">Training Datasets</a></li>
<li class="toc-entry toc-h3"><a href="#training">Training</a></li>
<li class="toc-entry toc-h3"><a href="#model-architecture">Model Architecture</a></li>
<li class="toc-entry toc-h3"><a href="#learning-philosophy">Learning Philosophy</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#applications">Applications</a>
<ul>
<li class="toc-entry toc-h3"><a href="#text-generation">Text Generation</a></li>
<li class="toc-entry toc-h3"><a href="#general-nlp-tasks">General NLP Tasks</a></li>
<li class="toc-entry toc-h3"><a href="#explaining-idioms">Explaining Idioms</a></li>
<li class="toc-entry toc-h3"><a href="#mind-blowing-search-engine">Mind-blowing search engine.</a></li>
<li class="toc-entry toc-h3"><a href="#layout-generator">Layout Generator</a></li>
<li class="toc-entry toc-h3"><a href="#the-debate-of-gpt-3-against-another-gpt-3">The debate of GPT-3 against another GPT-3!</a></li>
</ul>
</li>
</ul>
</li>
</ul><h1 id="gpt-3-the-model-simply-knows">
<a class="anchor" href="#gpt-3-the-model-simply-knows" aria-hidden="true"><span class="octicon octicon-link"></span></a>GPT-3, The Model Simply Knows!</h1>

<p><img src="/Analixa/images/GPT-3.gif" alt="" title="GPT-3, The Model Simply Knows">
Natural Language Processing (NLP) has been one of the most challenging areas in deep learning. This is due to several reasons. First, human language is complicated, even for humans themselves!</p>

<p>Consider asking someone about his experience in learning Chinese, for example. Without a doubt, he will tell you that this is difficult. The difficulty in learning any language is that almost all the meaning can be derived from the contextual conversational pipeline. In other words, you can’t tell what the purpose. Let’s see the following example:</p>

<ul>
  <li>The UN sanctioned Iran’s use of nuclear power (allowed)</li>
  <li>The UN agreed on sanctions for Iran because they used nuclear power (punishment)
This example is callous even for a native English speaker, and it would be tougher for a model to make some kind of reasoning for these two sentences.</li>
</ul>

<p>Second, language is handled as sequences that can vary in input or output length, which is another obstacle for the modeling process. Because the model simply does not know what is the most important word in a sequence that could predict the next works in a conversational pipeline.</p>

<p>Now, let’s see some of the revolutionary approaches in NLP that made a real leap!</p>

<h2 id="attention-is-all-you-need">
<a class="anchor" href="#attention-is-all-you-need" aria-hidden="true"><span class="octicon octicon-link"></span></a>Attention is All You Need</h2>
<p>This <a href="https://arxiv.org/abs/1706.03762">paper</a> was a turning point in all NLP. It solves most of the context problems. That means we can finally solve the problem of the most essential or related past words in predicting the next ones using a structured mathematical description. 
<img src="/Analixa/images/attention.png" alt="" title="Transformers Structure"></p>

<p>This paper laid the foundation for the use of <em>Transformers</em> (with attention heads) instead of <em>Sequence Models</em> (RNNs) only.</p>

<p>Afterward, these concepts paved the way towards a new breed of language models that are capable of reasoning in some sense.</p>

<h2 id="bert">
<a class="anchor" href="#bert" aria-hidden="true"><span class="octicon octicon-link"></span></a>BERT</h2>

<p>BERT which stands for Bidirectional Encoder Representations from
Transformers found in this <a href="https://arxiv.org/abs/1810.04805">paper</a> put the idea of transformers with attention heads at the top of language models. BERT was, breathtakingly, a model that marks a new era in language models. The idea behind BERT is simple as it constructs out of two main stages.</p>

<ul>
  <li>Pre-training, in which we just train our model on a large corpus to do some supervised tasks like next sentence prediction ( NSP ).</li>
  <li>Fine Tuning, another training step using a relatively smaller dataset to a more specialized task.<br>
<img src="images/bert.png" alt="BERT">
BERT, with its 345 million parameters, was able to achieve very superior performance over any model of its kind.</li>
</ul>

<h2 id="gpt-3">
<a class="anchor" href="#gpt-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>GPT-3</h2>

<p>A gigantic model, the largest model ever built by humans. GPT-3, presented by OpenAI in this <a href="https://arxiv.org/abs/2005.14165">paper</a>, is the state-of-the-art (SOTA) language model with a very superior model that is capable of doing the most NLP-related tasks.</p>

<p>Now, we will talk about different aspects of the model from the training data, the number of parameters, and the training procedures.</p>

<h3 id="training-datasets">
<a class="anchor" href="#training-datasets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training Datasets</h3>

<p><em>GPT-3</em> is trained on almost the entire internet or, according to my expression, trained on the whole human civilization.</p>

<p>The corpus is the largest ever collected for any language model ever. It’s kind of mind-blowing to be thinking about how enormous it is! The following table from the original paper depicts the datasets and their weight in the training mix.</p>

<p><img src="images/training.png" alt="Training Data">
These datasets would sum up to 500 billion tokens, which is incredibly massive and incomparable to any other language model.</p>

<p>In a nutshell, these datasets are forming the whole human knowledge! Almost every webpage you have seen in your life is included in this training process!
Let’s move on to the training procedure.</p>

<h3 id="training">
<a class="anchor" href="#training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training</h3>

<p>We are waiting for OpenAI to reveal more details about the training infrastructure and model implementation. But to put things into perspective, the GPT-3 175B model required 3.14E23 FLOPS of computing for training. Even at theoretical 28 TFLOPS for V100 and lowest three years reserved cloud pricing we could find, this will take 355 GPU-years and cost $4.6M for a single training run. Similarly, a single RTX 8000, assuming 15 TFLOPS, would take 665 years to run.</p>

<p>Time is not the only enemy. The 175 Billion parameters need 700GB memory to store. This is one order of magnitude larger than the maximum memory in a single GPU (48 GB of Quadro RTX 8000). To train the larger models without running out of memory, the OpenAI team uses a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU’s on the part of a high-bandwidth cluster provided by Microsoft.</p>

<p>The following graph is showing some information about the training power required to train different sizes of language models in Petaflop/s-days.</p>

<p><img src="images/training-time.png" alt="Training Time"></p>

<h3 id="model-architecture">
<a class="anchor" href="#model-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model Architecture</h3>

<p>GPT-3 comes in eight sizes, ranging from 125M to 175B parameters. The largest GPT-3 model is an order of magnitude larger than the previous record-holder, T5-11B. The smallest GPT-3 model is roughly the size of BERT-Base and RoBERTa-Base.</p>

<p>All GPT-3 models use the same attention-based architecture as their GPT-2 predecessor. The smallest GPT-3 model (125M) has 12 attention layers, each with 12x 64-dimension heads. The largest GPT-3 model (175B) uses 96 attention layers, each with 96x 128-dimension heads.</p>

<p>GPT-3 expanded the capacity of its GPT-2 by three orders of magnitudes without significant modification of the model architecture — just more layers, wider layers, and more data to train it on.</p>

<h3 id="learning-philosophy">
<a class="anchor" href="#learning-philosophy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learning Philosophy</h3>

<p>Unlike all previous models that are trained on next word prediction and then be fine-tuned on a specific task, GPT-3 is only trained on next word prediction only with no other fine-tuning on any task. However, GPT-3 is doing surprisingly well! Let’s dive into more details about this!</p>

<p><img src="images/learning.png" alt="learning">
As the comparison shows, the model is merely doing almost any task just by remembering it. Some tasks can be achieved and in an excellent performance with no remembering at all. These are zero-shot learning tasks. Others will require one example to follow, and those can be called one-shot learning tasks. Besides, the most difficult tasks that may require several cases to remember are called few-shot learning tasks.</p>

<p>Despite that this approach is straightforward, it’s useful! I think that every natural language task in the human brain is handled that way. For example, programmers, after spending some time in programming they can immediately think of code as a text completion in their minds, and that’s all.</p>

<p>The effect of this simple approach did not prove itself before. Only when having the required training data and the enormous model, it become crystal clear!</p>

<h2 id="applications">
<a class="anchor" href="#applications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Applications</h2>

<p>This may be the most breathtaking part about GPT-3. In the following passages, I will try to view most of these applications.</p>

<h3 id="text-generation">
<a class="anchor" href="#text-generation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Text Generation</h3>

<p>This is GPT’s rockstar application – a conditional generative model that creates near-human level quality text content. Given the beginning of some articles, the model is asked to generate the rest of the story in a word-by-word fashion.</p>

<p>More precisely, GPT-3 is presented with a title, a subtitle, and the prompt word “Article:” It then writes short articles (~200 words) that fools humans most of the time. According to OpenAI’s user study, “mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above change at ~52%”. Meaning humans will make random guesses while asking to detect GPT-3 generated articles. In contrast, the mean human accuracy at detecting particles produced by the smallest GPT-3 model (125M) is 76%.</p>

<p>This can be a big deal — “simply” increasing the size of the model by three orders of magnitude can change something that is half-working into something non-distinguishable from human work. In plain English, this empirically shows that the number of model parameters, the FLOP/s-days, and the number of training examples needs to grow according to a power function of the improvement of the model.</p>

<p>Of course, GPT-3 may still produce non-factual content (such as suggesting the popular U.S. TV program “The Tonight Show” is hosted by Megyn Kelly instead of Jimmy Fallon), nor did OpenAI claim the model is ready for writing the last two books of “A Song of Ice and Fire.” Nonetheless, getting closer to the finishing line of the Turing test for writing short articles is significant, and will no doubts have an enormous impact on our social media.</p>

<h3 id="general-nlp-tasks">
<a class="anchor" href="#general-nlp-tasks" aria-hidden="true"><span class="octicon octicon-link"></span></a>General NLP Tasks</h3>

<p>Although writing a new article is excellent, the killer feature of GPT-3 is the ability to be ‘re-programmed’ for general NLP tasks without any finetuning. This is where OpenAI’s real ambition lies: having a model to do just about anything by conditioning it with a few examples.</p>

<p>The paper showed a dozen of downstream tasks, ranging from the usual players such as machine translation and question and answer to the unexpected new tasks such as arithmetic computation and one-shot learning of new words. Instead of reiterating the details of each task, the rest of this article will discuss some common patterns across the board.</p>

<p>In the next lines, we will try to cover some of these cool applications.</p>

<h3 id="explaining-idioms">
<a class="anchor" href="#explaining-idioms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Explaining Idioms</h3>

<p>Just by using a few-shot learning approach, the model is surprisingly perfect in explaining idioms! See the conversation below, generated by GPT-3.</p>

<p><img src="images/explain.png" alt="Explaining Idioms"></p>
<h3 id="mind-blowing-search-engine">
<a class="anchor" href="#mind-blowing-search-engine" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mind-blowing search engine.</h3>

<p>It’s not like any search engine because you can ask it in plain English back in plain English also with no search results, only the right and exact answer!</p>

<p><img src="images/search-engine.gif" alt="Search Engine"></p>

<h3 id="layout-generator">
<a class="anchor" href="#layout-generator" aria-hidden="true"><span class="octicon octicon-link"></span></a>Layout Generator</h3>

<p>Imagine writing some English sentences and get back in return, a full functioning front-end app in a blink of an eye!</p>

<p><img src="images/layout.gif" alt="Layout Generator"></p>

<h3 id="the-debate-of-gpt-3-against-another-gpt-3">
<a class="anchor" href="#the-debate-of-gpt-3-against-another-gpt-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>The debate of GPT-3 against another GPT-3!</h3>

<p>What if you can see an entirely generated debate just be making a GPT-3 model talk with another! It’s incredible. What is more frightening is that these two models after five messages began to the limitations of human beings.</p>

<p><img src="images/debate.png" alt="Debate"></p>

<p>Finally, and without a doubt, GPT-3 has changed the face of language modeling from now on. This superior performance will open the door to dozens of philosophical and ethical questions. Some people may wonder about some jobs, such as will these models make some jobs disappear like writers, coders, and teachers. Is it ethical to use this model to pass exams?! ( Note that GPT-3 was able to score 57% in the SAT exam ). I think the future will answer all of these questions!</p>

<p>One more thing that made me can’t wait for the next GPT is that it will support interactive podcasts! And this is really beyond imagination!</p>


  </div><a class="u-url" href="/Analixa/markdown/2022/03/08/GPT-3,-The-Model-Simply-Knows.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Analixa/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Analixa/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Analixa/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Data Science School</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/aaabulkhair" target="_blank" title="aaabulkhair"><svg class="svg-icon grey"><use xlink:href="/Analixa/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/aaabulkhair_" target="_blank" title="aaabulkhair_"><svg class="svg-icon grey"><use xlink:href="/Analixa/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
