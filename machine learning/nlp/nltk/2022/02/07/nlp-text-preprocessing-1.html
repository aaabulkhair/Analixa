<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Text Preprocessing NLP 1 | Analixa</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Text Preprocessing NLP 1" />
<meta name="author" content="Ahmed Abulkhair" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this tutorial, I tried to demonstrate the basic concepts of preprocessing for natural text starting from stopwords removal, lowering, and lemmetization." />
<meta property="og:description" content="In this tutorial, I tried to demonstrate the basic concepts of preprocessing for natural text starting from stopwords removal, lowering, and lemmetization." />
<link rel="canonical" href="https://aaabulkhair.github.io/Analixa/machine%20learning/nlp/nltk/2022/02/07/nlp-text-preprocessing-1.html" />
<meta property="og:url" content="https://aaabulkhair.github.io/Analixa/machine%20learning/nlp/nltk/2022/02/07/nlp-text-preprocessing-1.html" />
<meta property="og:site_name" content="Analixa" />
<meta property="og:image" content="https://aaabulkhair.github.io/Analixa/images/nlp1.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-07T00:00:00-06:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://aaabulkhair.github.io/Analixa/images/nlp1.png" />
<meta property="twitter:title" content="Text Preprocessing NLP 1" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ahmed Abulkhair"},"dateModified":"2022-02-07T00:00:00-06:00","datePublished":"2022-02-07T00:00:00-06:00","description":"In this tutorial, I tried to demonstrate the basic concepts of preprocessing for natural text starting from stopwords removal, lowering, and lemmetization.","headline":"Text Preprocessing NLP 1","image":"https://aaabulkhair.github.io/Analixa/images/nlp1.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://aaabulkhair.github.io/Analixa/machine%20learning/nlp/nltk/2022/02/07/nlp-text-preprocessing-1.html"},"url":"https://aaabulkhair.github.io/Analixa/machine%20learning/nlp/nltk/2022/02/07/nlp-text-preprocessing-1.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Analixa/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://aaabulkhair.github.io/Analixa/feed.xml" title="Analixa" /><link rel="shortcut icon" type="image/x-icon" href="/Analixa/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Analixa/">Analixa</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Analixa/about/">About Me</a><a class="page-link" href="/Analixa/search/">Search</a><a class="page-link" href="/Analixa/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Text Preprocessing | NLP 1</h1><p class="page-description">In this tutorial, I tried to demonstrate the basic concepts of `preprocessing` for natural text starting from `stopwords removal`, `lowering`, and `lemmetization`.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-07T00:00:00-06:00" itemprop="datePublished">
        Feb 7, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Ahmed Abulkhair</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      12 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Analixa/categories/#Machine Learning">Machine Learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/Analixa/categories/#NLP">NLP</a>
        &nbsp;
      
        <a class="category-tags-link" href="/Analixa/categories/#NLTK">NLTK</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/aaabulkhair/Analixa/tree/master/_notebooks/2022-02-07-nlp-text-preprocessing-1.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/Analixa/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/aaabulkhair/Analixa/master?filepath=_notebooks%2F2022-02-07-nlp-text-preprocessing-1.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Analixa/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/aaabulkhair/Analixa/blob/master/_notebooks/2022-02-07-nlp-text-preprocessing-1.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Analixa/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Faaabulkhair%2FAnalixa%2Fblob%2Fmaster%2F_notebooks%2F2022-02-07-nlp-text-preprocessing-1.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/Analixa/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#Introduction">Introduction </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Algorithms-can-not-comprehend-text,-only-numbers!">Algorithms can not comprehend text, only numbers! </a></li>
<li class="toc-entry toc-h2"><a href="#Setup">Setup </a></li>
<li class="toc-entry toc-h2"><a href="#About-the-Twitter-dataset">About the Twitter dataset </a></li>
<li class="toc-entry toc-h2"><a href="#Looking-at-raw-texts">Looking at raw texts </a></li>
<li class="toc-entry toc-h2"><a href="#Steps-of-Preprocessing">Steps of Preprocessing </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Removing-unnesseary-parts-(mentions,-tags,-hashtags,-and-URLs)">Removing unnesseary parts (mentions, tags, hashtags, and URLs) </a></li>
<li class="toc-entry toc-h3"><a href="#Lowercasing">Lowercasing </a></li>
<li class="toc-entry toc-h3"><a href="#Tokenization">Tokenization </a></li>
<li class="toc-entry toc-h3"><a href="#Removing-stop-words-and-punctuation">Removing stop words and punctuation </a></li>
<li class="toc-entry toc-h3"><a href="#Stemming-and/or-lemmetization">Stemming and/or lemmetization </a></li>
<li class="toc-entry toc-h3"><a href="#Putting-it-all-together.">Putting it all together. </a></li>
</ul>
</li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-02-07-nlp-text-preprocessing-1.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h1>
<p>When dealing with text data with Machine Learning models. There's a huge and simple problem! Which is,</p>
<h4 id="Algorithms-can-not-comprehend-text,-only-numbers!">
<a class="anchor" href="#Algorithms-can-not-comprehend-text,-only-numbers!" aria-hidden="true"><span class="octicon octicon-link"></span></a><em>Algorithms can not comprehend text, only numbers!</em><a class="anchor-link" href="#Algorithms-can-not-comprehend-text,-only-numbers!"> </a>
</h4>
<p>So, is it all about converting text into numbers? Of course, not! The natural language is a very senstive and expressive type of data which means alot of expressions and desires can be hidden in a piece of text an numbers are less likely to demonestrate that! Numbers can reveal very little information when comparing to the natural text. so, it this set of tutorials, we will try to handle most of the <code>preprocessing</code> techniques. Strating from the very basic (like what we will do in this tutorial) ending up with very complex.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup">
<a class="anchor" href="#Setup" aria-hidden="true"><span class="octicon octicon-link"></span></a>Setup<a class="anchor-link" href="#Setup"> </a>
</h2>
<p>We will be using the <a href="http://www.nltk.org/howto/twitter.html">Natural Language Toolkit (NLTK)</a> package and <a href="https://spacy.io/">SpaCy</a>, open-source Python libraries for natural language processing. NLTK has modules for collecting, handling, and processing Twitter data.</p>
<p>For this tutorial, we will use a Twitter dataset that comes with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. Let us import them now as well as a few other libraries we will be using.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>                                <span class="c1"># Python library for NLP</span>
<span class="kn">import</span> <span class="nn">spacy</span>                               <span class="c1"># Python libray for NLP</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">twitter_samples</span>    <span class="c1"># sample Twitter dataset from NLTK</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>            <span class="c1"># library for visualization</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>                      <span class="c1"># library for visualization</span>
<span class="kn">import</span> <span class="nn">random</span>                              <span class="c1"># pseudo-random number generator</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="About-the-Twitter-dataset">
<a class="anchor" href="#About-the-Twitter-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>About the Twitter dataset<a class="anchor-link" href="#About-the-Twitter-dataset"> </a>
</h2>
<p>The sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial.</p>
<p>You can download the dataset in your workspace by doing:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'twitter_samples'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[nltk_data] Downloading package twitter_samples to
[nltk_data]     /usr/share/nltk_data...
[nltk_data]   Package twitter_samples is already up-to-date!
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>True</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can load the text fields of the positive and negative tweets by using the module's <code>strings()</code> method like this:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">all_positive_tweets</span> <span class="o">=</span> <span class="n">twitter_samples</span><span class="o">.</span><span class="n">strings</span><span class="p">(</span><span class="s1">'positive_tweets.json'</span><span class="p">)</span>
<span class="n">all_negative_tweets</span> <span class="o">=</span> <span class="n">twitter_samples</span><span class="o">.</span><span class="n">strings</span><span class="p">(</span><span class="s1">'negative_tweets.json'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we'll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'Number of positive tweets: '</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_positive_tweets</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Number of negative tweets: '</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_negative_tweets</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">The type of all_positive_tweets is: '</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">all_positive_tweets</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'The type of a tweet entry is: '</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">all_negative_tweets</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of positive tweets:  5000
Number of negative tweets:  5000

The type of all_positive_tweets is:  &lt;class 'list'&gt;
The type of a tweet entry is:  &lt;class 'str'&gt;
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, lets construct a <code>pie chart</code> to point out class distrbution.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">colors</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s1">'pastel'</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>

<span class="c1">#create pie chart</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pie</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">all_positive_tweets</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_negative_tweets</span><span class="p">)],</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Positive'</span><span class="p">,</span> <span class="s1">'Negative'</span><span class="p">],</span> <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span><span class="p">,</span> <span class="n">autopct</span><span class="o">=</span><span class="s1">'</span><span class="si">%.0f%%</span><span class="s1">'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Looking-at-raw-texts">
<a class="anchor" href="#Looking-at-raw-texts" aria-hidden="true"><span class="octicon octicon-link"></span></a>Looking at raw texts<a class="anchor-link" href="#Looking-at-raw-texts"> </a>
</h2>
<p>Before anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we'd like to consider when preprocessing our data.</p>
<p>Below, you will print one random positive and one random negative tweet. We have added a color mark at the beginning of the string to further distinguish the two. (Warning: This is taken from a public dataset of real tweets and a very small portion has explicit content.)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[92m'</span> <span class="o">+</span> <span class="n">all_positive_tweets</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5000</span><span class="p">)])</span>

<span class="c1"># print negative in red</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[91m'</span> <span class="o">+</span> <span class="n">all_negative_tweets</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5000</span><span class="p">)])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre><span class="ansi-green-intense-fg">@PuppyShogun mistakes happen man, as long as we get to play the game, we'll be happy :)
</span><span class="ansi-red-intense-fg">＠maverickgamer_　July 24, 2015 at 07:17PM 　:(
</span></pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One observation, is the presence of emoticons and URLs in many of the tweets. This info will come in handy in the next steps.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Steps-of-Preprocessing">
<a class="anchor" href="#Steps-of-Preprocessing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Steps of Preprocessing<a class="anchor-link" href="#Steps-of-Preprocessing"> </a>
</h2>
<p>Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:</p>
<ul>
<li>Removing unnesseary parts (mentions, tags, hashtags, and URLs)</li>
<li>Lowercasing</li>
<li>Removing stop words and punctuation</li>
<li>Tokenizing the string</li>
<li>Stemming and/or lemmetization</li>
</ul>
<p>Besides, the first purpose of preprocessing which is <code>converting text into numbers</code>, there's one more important purpose, which is about reducing the redundency of the data size itself. In other words, we want to represent the text features in the most minimal feature space and this will be demonestrated clearly for each step.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's import some libraries to help us out!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>                                  <span class="c1"># library for regular expression operations</span>
<span class="kn">import</span> <span class="nn">string</span>                              <span class="c1"># for string operations</span>
<span class="kn">import</span> <span class="nn">spacy</span>                               <span class="c1"># Text processing</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tweet</span> <span class="o">=</span> <span class="n">all_positive_tweets</span><span class="p">[</span><span class="mi">2277</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Removing-unnesseary-parts-(mentions,-tags,-hashtags,-and-URLs)">
<a class="anchor" href="#Removing-unnesseary-parts-(mentions,-tags,-hashtags,-and-URLs)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Removing unnesseary parts (mentions, tags, hashtags, and URLs)<a class="anchor-link" href="#Removing-unnesseary-parts-(mentions,-tags,-hashtags,-and-URLs)"> </a>
</h3>
<p>Since we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We'll use the <a href="https://docs.python.org/3/library/re.html">re</a> library to perform regular expression operations on our tweet. We'll define our search pattern and use the <code>sub()</code> method to remove matches by substituting with an empty character (i.e. <code>''</code>)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">RT_remover</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">'^b\s([RT]+)?'</span><span class="p">,</span><span class="s1">''</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="c1"># remove all URLs</span>
<span class="n">URL_remover</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">'http\S+'</span><span class="p">,</span> <span class="s1">''</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="c1"># remove hashtags</span>
<span class="c1"># only removing the hash # sign from the word</span>
<span class="n">Hashtag_remover</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">'#'</span><span class="p">,</span> <span class="s1">''</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Apply all functions</span>
<span class="n">tweet</span> <span class="o">=</span> <span class="n">RT_remover</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
<span class="n">tweet</span> <span class="o">=</span> <span class="n">URL_remover</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
<span class="n">tweet</span> <span class="o">=</span> <span class="n">Hashtag_remover</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>


<span class="c1"># print final output</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… 
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you need any further info about using and testing regex, you may visit this <a href="https://regex101.com/">website</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Lowercasing">
<a class="anchor" href="#Lowercasing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lowercasing<a class="anchor-link" href="#Lowercasing"> </a>
</h3>
<p>This step is often done for the sake of <code>reducing the feature representation space</code>.</p>
<p>For example:</p>
<ul>
<li>I love cars! Cars are the most passionate hoppy in the world.</li>
</ul>
<blockquote>
<p>Without <code>lowercasing</code>, <code>cars</code> and <code>Cars</code> with be two different words with two different representation which is not true nor optimal for sure.</p>
</blockquote>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lowercase</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="c1"># Apply function</span>
<span class="n">tweet</span> <span class="o">=</span> <span class="n">lowercase</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
<span class="c1"># Print result</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>my beautiful sunflowers on a sunny friday morning off :) sunflowers favourites happy friday off… 
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tokenization">
<a class="anchor" href="#Tokenization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tokenization<a class="anchor-link" href="#Tokenization"> </a>
</h3>
<p>Although this word seems to be complex but it means a very simple process which is <code>dividing the natural text into parts</code>! This will make the process of generating a feature representation much easier.</p>
<p>Here, we will divide our natural text into words which are the basic building block of a text. We will be doing that using some built-in functions such as <code>split()</code> and <code>strip()</code></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    transform text into list of tokens</span>
<span class="sd">    - param:</span>
<span class="sd">        - text: input text -&gt; str</span>
<span class="sd">    - return</span>
<span class="sd">        - text_tokens: list of tokens</span>
<span class="sd">    """</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="n">text_tokens</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">text_tokens</span>

<span class="c1"># test the function</span>
<span class="n">tweet_tokens</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
<span class="n">tweet_tokens</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>['my',
 'beautiful',
 'sunflowers',
 'on',
 'a',
 'sunny',
 'friday',
 'morning',
 'off',
 ':)',
 'sunflowers',
 'favourites',
 'happy',
 'friday',
 'off…']</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Removing-stop-words-and-punctuation">
<a class="anchor" href="#Removing-stop-words-and-punctuation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Removing stop words and punctuation<a class="anchor-link" href="#Removing-stop-words-and-punctuation"> </a>
</h3>
<p>Stop words and punctuation don't give a lot of meaning in most cases. Let's see an example.</p>
<p><code>There is a ball on the table. It is huge and colorful!</code></p>
<p>Lets remove stop words and punctuation and see if we can capture the same meaning.</p>
<p><code>ball table huge colorful</code></p>
<p>As you can see, it's much shorter and this will defintly lead to lower feature representation. Some packages have provided lists for the stopwords and punctuation in many languges. In this tutorial, we will be using <a href="https://spacy.io/">SpaCy</a> and <code>String</code> packages.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">en</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'en_core_web_sm'</span><span class="p">)</span>
<span class="c1"># Donwload Stopwords</span>
<span class="n">stopwords</span> <span class="o">=</span> <span class="n">en</span><span class="o">.</span><span class="n">Defaults</span><span class="o">.</span><span class="n">stop_words</span>
<span class="c1"># Convert to list</span>
<span class="n">lst_stopwords</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">stopwords</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s1">'Stop words</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lst_stopwords</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">Punctuation</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Stop words

['‘s', 'even', 'everything', 'how', 'thereafter', 'being', 'mine', 'up', 'make', 'really', '’d', 'own', 'becoming', 'none', 'but', 'between', 'down', '’s', 'whoever', 'been', 'every', 'else', '’m', 'unless', 'serious', 'ever', 'those', 'above', 'wherever', 'cannot', 'any', 'back', 'here', 'whom', 'to', 'well', 'fifteen', 'each', 'due', 'eight', 'whenever', 'often', 'last', 'both', 'ca', 'throughout', 'go', 'mostly', 'three', 'before', 'or', 'few', 'for', 'such', '’re', 'would', 'done', 'thru', 'almost', 'eleven', 'other', 'do', 'this', 'hereby', 'along', 'n‘t', "'d", 'next', 'one', 'seeming', 'were', 'then', 'yourself', 'part', 'wherein', 'while', 'anywhere', 'twenty', 'her', 'beforehand', 'put', 'somehow', 'off', 'either', 'very', 'us', 'perhaps', 'over', 'various', 'no', 'also', 'thus', 'via', 'ours', "'ll", 'someone', 'onto', 'ten', 'just', 'further', 'had', 'within', 'a', 'nine', 'him', 'his', 'never', 'third', 'full', 'nothing', 'hundred', 'there', 'hers', 'around', 'indeed', 'now', 'can', 'nevertheless', '‘re', 'therein', 'seem', 'twelve', 'thereby', 'might', 'from', 'others', 'hereupon', 'an', 'again', 'made', 'once', 'though', 'five', 'by', '‘ll', 'at', 'several', 'whither', 'their', 'our', 'least', 'some', 'too', 'the', 'they', 'except', 'who', 'thereupon', 'meanwhile', 'side', 'you', 'another', 'elsewhere', 'first', 'per', 'yet', 'formerly', '‘m', 'whence', 'latterly', 'whether', 'used', 'across', 'he', 'enough', 'will', 'whole', "n't", 'all', 'we', 'fifty', 'always', 'after', 'i', 'bottom', 'himself', 'n’t', 'afterwards', 'upon', 'with', 'ourselves', 'my', 'therefore', "'s", 'most', 'top', 'under', 'your', 'not', 'nowhere', 'anyway', 'whatever', 'anyone', '‘ve', 'through', 'rather', 'whereafter', 'seemed', 'so', 'get', 'nor', 'against', 'moreover', 'yourselves', 'anyhow', 'about', 'move', 'was', 'yours', 'because', 'below', 'six', 'latter', 'everyone', 'noone', 'still', 'does', 'must', 'them', 'call', 'why', '‘d', 'since', 'amount', 'alone', 'regarding', 'became', 'and', 'may', 'former', 'myself', 'doing', 'less', 'she', 'into', '’ll', 'sometime', 'although', 'name', 'itself', 'herself', 'besides', 'it', 'as', 'when', 'thence', 'if', 'front', 'seems', 'whereas', 'give', 'these', 'empty', 'themselves', 'many', 'see', 'much', 'become', 'only', 'forty', 'could', 'that', 'what', 'otherwise', 're', 'on', 'say', 'sometimes', 'amongst', 'in', 'namely', 'together', 'please', 'which', 'towards', 'without', 'hereafter', 'during', 'everywhere', 'has', 'out', 'whose', "'re", 'more', "'ve", 'am', 'whereby', 'nobody', 'beside', 'somewhere', 'quite', 'toward', 'me', '’ve', 'herein', 'using', 'beyond', 'have', 'among', 'than', "'m", 'where', 'did', 'until', 'sixty', 'whereupon', 'are', 'its', 'however', 'becomes', 'behind', 'something', 'neither', 'same', 'already', 'four', 'two', 'is', 'show', 'keep', 'of', 'anything', 'should', 'hence', 'be', 'take']

Punctuation

!"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`{|}~
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tweets_clean</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="c1"># clean all word tokens</span>
<span class="k">for</span> <span class="n">wrd</span> <span class="ow">in</span> <span class="n">tweet_tokens</span><span class="p">:</span>                   <span class="c1"># Iterate over all words</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">wrd</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lst_stopwords</span> <span class="ow">and</span>
        <span class="n">wrd</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">)):</span>
        <span class="n">tweets_clean</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wrd</span><span class="p">)</span>           <span class="c1"># Add to clean tweets list</span>

<span class="n">tweets_clean</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>['beautiful',
 'sunflowers',
 'sunny',
 'friday',
 'morning',
 ':)',
 'sunflowers',
 'favourites',
 'happy',
 'friday',
 'off…']</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Stemming-and/or-lemmetization">
<a class="anchor" href="#Stemming-and/or-lemmetization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stemming and/or lemmetization<a class="anchor-link" href="#Stemming-and/or-lemmetization"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.</p>
<p>Consider the words:</p>
<ul>
<li><strong>learn</strong></li>
<li>
<strong>learn</strong>ing</li>
<li>
<strong>learn</strong>ed</li>
<li>
<strong>learn</strong>t</li>
</ul>
<p>All these words are stemmed from its common root <strong>learn</strong>. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, <strong>happi</strong> and <strong>sunni</strong>. That's because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:</p>
<ul>
<li>
<strong>happ</strong>y</li>
<li>
<strong>happi</strong>ness</li>
<li>
<strong>happi</strong>er</li>
</ul>
<p>We can see that the prefix <strong>happi</strong> is more commonly used. We cannot choose <strong>happ</strong> because it is the stem of unrelated words like <strong>happen</strong>.</p>
<p>Lemmetization is the process of coverting a word into the least meaningfull part of it. Unlike stemming, lemmetization has to produce the least correct root with correct spelling.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Word</th>
<th style="text-align:right">Lemma</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>happiness</strong></td>
<td style="text-align:right"><strong>happiness</strong></td>
</tr>
<tr>
<td style="text-align:left"><strong>happier</strong></td>
<td style="text-align:right"><strong>happy</strong></td>
</tr>
<tr>
<td style="text-align:left"><strong>happier</strong></td>
<td style="text-align:right"><strong>happy</strong></td>
</tr>
</tbody>
</table>
<p>Now, lets see some words that might make you feel the differnce.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Word</th>
<th style="text-align:right">Stem</th>
<th style="text-align:center">Lemma</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>caring</strong></td>
<td style="text-align:right"><strong>care</strong></td>
<td style="text-align:center"><strong>caring</strong></td>
</tr>
<tr>
<td style="text-align:left"><strong>ties</strong></td>
<td style="text-align:right"><strong>ti</strong></td>
<td style="text-align:center"><strong>tie</strong></td>
</tr>
<tr>
<td style="text-align:left"><strong>easily</strong></td>
<td style="text-align:right"><strong>easili</strong></td>
<td style="text-align:center"><strong>easily</strong></td>
</tr>
<tr>
<td style="text-align:left"><strong>mice</strong></td>
<td style="text-align:right"><strong>mice</strong></td>
<td style="text-align:center"><strong>mouse</strong></td>
</tr>
</tbody>
</table>
<p>As you can see, stemming can sometimes produce uncorrect words such as <code>ti</code> and <code>easili</code>. On contrary, the lemmetization always produce a correct word in spelling even if this will lead to higher feature representation. For example, <code>caring</code> and <code>care</code> will be to different elements in the feature representation, however they share the same root <code>car</code> when dealing with stemming instead. We will be using <code>NLTK</code> library for that purpose for lemmetizing this data.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
<span class="c1"># Intiate WordNetLemmatizer</span>
<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="c1"># Lemmetize the words</span>
<span class="n">tweets_lemmas</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">wrd</span> <span class="ow">in</span> <span class="n">tweets_clean</span><span class="p">:</span>
    <span class="n">wrd</span> <span class="o">=</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">wrd</span><span class="p">)</span>
    <span class="n">tweets_lemmas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wrd</span><span class="p">)</span>
    
<span class="n">tweets_lemmas</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>['beautiful',
 'sunflower',
 'sunny',
 'friday',
 'morning',
 ':)',
 'sunflower',
 'favourite',
 'happy',
 'friday',
 'off…']</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Putting-it-all-together.">
<a class="anchor" href="#Putting-it-all-together." aria-hidden="true"><span class="octicon octicon-link"></span></a>Putting it all together.<a class="anchor-link" href="#Putting-it-all-together."> </a>
</h3>
<p>By now, we have completed all the required preprocessing steps. Lets combine them all in a one preprocess function that will be used in our next tutorial.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    
    <span class="c1"># remove old style retweet text "RT"</span>
    <span class="n">RT_remover</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">'^b\s([RT]+)?'</span><span class="p">,</span><span class="s1">''</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="c1"># remove all URLs</span>
    <span class="n">URL_remover</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">'http\S+'</span><span class="p">,</span> <span class="s1">''</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="c1"># remove hashtags</span>
    <span class="c1"># only removing the hash # sign from the word</span>
    <span class="n">Hashtag_remover</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">'#'</span><span class="p">,</span> <span class="s1">''</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="c1"># Apply all functions</span>
    <span class="n">txt</span> <span class="o">=</span> <span class="n">RT_remover</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span>
    <span class="n">txt</span> <span class="o">=</span> <span class="n">URL_remover</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span>
    <span class="n">txt</span> <span class="o">=</span> <span class="n">Hashtag_remover</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">"""</span>
<span class="sd">        transform text into list of tokens</span>
<span class="sd">        - param:</span>
<span class="sd">            - text: input text -&gt; str</span>
<span class="sd">        - return</span>
<span class="sd">            - text_tokens: list of tokens</span>
<span class="sd">        """</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">text_tokens</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">text_tokens</span>
    
    <span class="n">lst_txt</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span>
    
    <span class="c1"># Import modules</span>
    <span class="kn">import</span> <span class="nn">string</span>
    <span class="kn">import</span> <span class="nn">spacy</span>
    <span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
    <span class="c1"># Load the core utils for english languge</span>
    <span class="n">en</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'en_core_web_sm'</span><span class="p">)</span>
    <span class="c1"># Donwload Stopwords</span>
    <span class="n">stopwords</span> <span class="o">=</span> <span class="n">en</span><span class="o">.</span><span class="n">Defaults</span><span class="o">.</span><span class="n">stop_words</span>
    <span class="c1"># Convert to list</span>
    <span class="n">lst_stopwords</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">stopwords</span><span class="p">)</span>
    
    <span class="n">lst_txt_clean</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="c1"># clean all word tokens</span>
    <span class="k">for</span> <span class="n">wrd</span> <span class="ow">in</span> <span class="n">lst_txt</span><span class="p">:</span>                   <span class="c1"># Iterate over all words</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">wrd</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lst_stopwords</span> <span class="ow">and</span>
            <span class="n">wrd</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">)):</span>
            <span class="n">lst_txt_clean</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wrd</span><span class="p">)</span>           <span class="c1"># Add to clean tweets list</span>

    <span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
    <span class="c1"># Lemmetize the words</span>
    <span class="n">lst_txt_lemmas</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">wrd</span> <span class="ow">in</span> <span class="n">lst_txt_clean</span><span class="p">:</span>
        <span class="n">wrd</span> <span class="o">=</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">wrd</span><span class="p">)</span>
        <span class="n">lst_txt_lemmas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wrd</span><span class="p">)</span>
        
        
    <span class="k">return</span> <span class="n">lst_txt_lemmas</span>

    
    
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's test it!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_tweet</span> <span class="o">=</span> <span class="n">all_positive_tweets</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5000</span><span class="p">)]</span>
<span class="n">preprocess</span><span class="p">(</span><span class="n">sample_tweet</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>['Well', 'morning', 'carry', 'great.', 'Work', ':)']</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That is the end of this tutorial! If you find this beneficial. Follow me for more updates!</p>
<ul>
<li><a href="https://www.linkedin.com/in/aaabulkhair/">LinkedIn</a></li>
<li><a href="https://www.kaggle.com/ahmedabdulhamid">Kaggle</a></li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="aaabulkhair/Analixa"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/Analixa/machine%20learning/nlp/nltk/2022/02/07/nlp-text-preprocessing-1.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Analixa/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Analixa/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Analixa/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Data Science School</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/aaabulkhair" target="_blank" title="aaabulkhair"><svg class="svg-icon grey"><use xlink:href="/Analixa/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/aaabulkhair_" target="_blank" title="aaabulkhair_"><svg class="svg-icon grey"><use xlink:href="/Analixa/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
