{
  
    
        "post0": {
            "title": "Titanic | The Power of Sklearn",
            "content": "Introduction . Neither Titanic dataset nor sklearn a new thing for any data scientist but there are some important features in scikit-learn that will make any model preprocessing and tuning easier, to be specific this notebook will cover the following concepts: . ColumnTransformer | Pipeline | SimpleImputer | StandardScalar | OneHotEncoder | OrdinalEncoder | GridSearch | . Note, this tutorial is a solution to the famous kaggle competition Titanic - Machine Learning from Disaster . Mounting Filesystem . import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # Any results you write to the current directory are saved as output. . /kaggle/input/titanic/gender_submission.csv /kaggle/input/titanic/test.csv /kaggle/input/titanic/train.csv . Import Packages . import pandas as pd # Numpy for Numerical operations import numpy as np # Import ColumnTransformer from sklearn.compose import ColumnTransformer # Import Pipeline from sklearn.pipeline import Pipeline # Import SimpleImputer from sklearn.impute import SimpleImputer # Import StandardScaler, OneHotEncodr and OrdinalEncoder from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder # Import Random Forest for Classification from sklearn.ensemble import RandomForestClassifier # Import GridSearch from sklearn.model_selection import GridSearchCV . Reading Data . In the following cells, we will read the train and test data and check for NaNs. . train_data = pd.read_csv(&quot;/kaggle/input/titanic/train.csv&quot;) # See some info train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): PassengerId 891 non-null int64 Survived 891 non-null int64 Pclass 891 non-null int64 Name 891 non-null object Sex 891 non-null object Age 714 non-null float64 SibSp 891 non-null int64 Parch 891 non-null int64 Ticket 891 non-null object Fare 891 non-null float64 Cabin 204 non-null object Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB . It&#39;s obvious that we had to deal with NaNs . test_data = pd.read_csv(&quot;/kaggle/input/titanic/test.csv&quot;) test_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): PassengerId 418 non-null int64 Pclass 418 non-null int64 Name 418 non-null object Sex 418 non-null object Age 332 non-null float64 SibSp 418 non-null int64 Parch 418 non-null int64 Ticket 418 non-null object Fare 417 non-null float64 Cabin 91 non-null object Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB . Splitting Data . X_train = train_data.drop([&#39;Survived&#39;, &#39;Name&#39;], axis = 1) X_test = test_data.drop([&#39;Name&#39;], axis = 1) y_train = train_data[&#39;Survived&#39;] . Continuous and Numerical features handling . It&#39;s clear that we have some numerical features that have some missing values to be imputed and they have to be of the same scale also. . In the following cell, we will handle the numerical features separtely i.e &quot;Age&quot; and &quot;Fare&quot; . # Difine a list with the numeric features numeric_features = [&#39;Age&#39;, &#39;Fare&#39;] # Define a pipeline for numer&quot;ic features numeric_features_pipeline = Pipeline(steps= [ (&#39;imputer&#39;, SimpleImputer(strategy = &#39;median&#39;)), # Impute with median value for missing (&#39;scaler&#39;, StandardScaler()) # Conduct a scaling step ]) . Categorical features handling . It&#39;s clear that we have some categorical features that have some missing values to be imputed and they have to be encoded using one hot encoding. . In the following cell, we will handle the categorical features separtely i.e &quot;Embarked&quot; and &quot;Sex&quot; . Note: I choose simple imputer for the missing cells to impute with &#39;missing&#39; word. My aim was to gather all missing cells in one category for further encoding. . # Difine a list with the categorical features categorical_features = [&#39;Embarked&#39;, &#39;Sex&#39;] # Define a pipeline for categorical features categorical_features_pipeline = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value = &#39;missing&#39;)), # Impute with the word &#39;missing&#39; for missing values (&#39;onehot&#39;, OneHotEncoder(handle_unknown = &#39;ignore&#39;)) # Convert all categorical variables to one hot encoding ]) . Ordinal features handling . Passenger class or &#39;Pclass&#39; for short is an ordinal feature that must be handled keeping in mind that class 3 is much higher than 2 and so on. . # Define a list with the ordinal features ordinal_features = [&#39;Pclass&#39;] # Define a pipline for ordinal features ordinal_features_pipeline = Pipeline(steps=[ (&#39;ordinal&#39;, OrdinalEncoder(categories= [[1, 2, 3]])) ]) . Construct a comprehended preprocessor . Now, we will create a preprocessor that can handle all columns in our dataset using ColumnTransformer . preprocessor = ColumnTransformer(transformers= [ # transformer with name &#39;num&#39; that will apply # &#39;numeric_features_pipeline&#39; to numeric_features (&#39;num&#39;, numeric_features_pipeline, numeric_features), # transformer with name &#39;cat&#39; that will apply # &#39;categorical_features_pipeline&#39; to categorical_features (&#39;cat&#39;, categorical_features_pipeline, categorical_features), # transformer with name &#39;ord&#39; that will apply # &#39;ordinal_features_pipeline&#39; to ordinal_features (&#39;ord&#39;, ordinal_features_pipeline, ordinal_features) ]) . Prediction Pipeline . Now, we will create a full prediction pipeline that uses our preprocessor and then transfer it to our classifier of choice &#39;Random Forest&#39;. . clf = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;classifier&#39;, RandomForestClassifier(n_estimators = 120, max_leaf_nodes = 100))]) . Pipeline Training . Let&#39;s train our pipeline now . clf.fit(X_train, y_train) . Pipeline(memory=None, steps=[(&#39;preprocessor&#39;, ColumnTransformer(n_jobs=None, remainder=&#39;drop&#39;, sparse_threshold=0.3, transformer_weights=None, transformers=[(&#39;num&#39;, Pipeline(memory=None, steps=[(&#39;imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy=&#39;median&#39;, verbose=0)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean... RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=100, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=120, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False))], verbose=False) . Pipeline Tuning . The question now, can we push it a little bit further? i.e. can we tune every single part or our Pipeline? . Here, I will use GridSearch to decide three things: . Simple Imputer strategy :mean or median&gt; - n_estimators of Random Forest | max leaf nodes of Random Forest | . Note, you can access any parameter from the outer level to the next adjacent inner one . For Example:to access the strategy of the Simple Imputer you can do the followingpreprocessornumimputer__strategy . Let&#39;s see this into action . param_grid = { &#39;preprocessor__num__imputer__strategy&#39;: [&#39;mean&#39;, &#39;median&#39;], &#39;classifier__n_estimators&#39;: [100, 120, 150, 170, 200], &#39;classifier__max_leaf_nodes&#39; : [100, 120, 150, 170, 200] } grid_search = GridSearchCV(clf, param_grid, cv=10) grid_search.fit(X_train, y_train) print((&quot;best random forest from grid search: %.3f&quot; % grid_search.score(X_train, y_train))) print(&#39;The best parameters of Simple Imputer and C are:&#39;) print(grid_search.best_params_) . best random forest from grid search: 0.944 The best parameters of Simple Imputer and C are: {&#39;classifier__max_leaf_nodes&#39;: 100, &#39;classifier__n_estimators&#39;: 150, &#39;preprocessor__num__imputer__strategy&#39;: &#39;median&#39;} . Generate Predictions . Let&#39;s generate predictions now using our grid search model and submit the results . predictions = grid_search.predict(X_test) # Generate results dataframe results_df = pd.DataFrame({&#39;PassengerId&#39;: test_data.PassengerId, &#39;Survived&#39;: predictions}) # Save to csv file results_df.to_csv(&#39;submission.csv&#39;, index = False) print(&#39;Submission CSV has been saved!&#39;) . Submission CSV has been saved! .",
            "url": "https://aaabulkhair.github.io/Analixa/machine%20learning/sklearn/python/2022/02/05/titanic-power-of-scikit-learn.html",
            "relUrl": "/machine%20learning/sklearn/python/2022/02/05/titanic-power-of-scikit-learn.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "&gt; RandomForest is always an easy-to-go algorithm but determining the best n_estimators can be very computationally intensive. In this tutorial, we will find a way to detrmine the best n_estimators without re - toc:true - branch: master - badges: true - comments: true - author: Ahmed Abulkhair - categories: [Machine Learning, Sklearn, Python] - image: images/titanic.png . In this notebook, we will try to determine the best number of n_estimators for RandomForest model without training the model for multiple times . Load Dataset . We will use one of the built-in datasets, which is digits . import sklearn.datasets from sklearn.model_selection import train_test_split # Load dataset X, y = sklearn.datasets.load_digits(n_class = 10,return_X_y = True) # Split the data X_train, X_val, y_train, y_val = train_test_split(X, y) . Import libraries . import numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score . Step 1: first fit a Random Forest to the data. Set n_estimators to a high value. . rf = RandomForestClassifier(n_estimators=500, max_depth=4, n_jobs=-1) rf.fit(X_train, y_train) . RandomForestClassifier(max_depth=4, n_estimators=500, n_jobs=-1) . Step 2: Get predictions for each tree in Random Forest separately. . predictions = [] for tree in rf.estimators_: predictions.append(tree.predict_proba(X_val)[None, :]) . Step 3: Concatenate the predictions to a tensor of size (number of trees, number of objects, number of classes). . predictions = np.vstack(predictions) . Step 4: Сompute cumulative average of the predictions. That will be a tensor, that will contain predictions of the random forests for each n_estimators. . cum_mean = np.cumsum(predictions, axis=0)/np.arange(1, predictions.shape[0] + 1)[:, None, None] . Step 5: Get accuracy scores for each n_estimators value . scores = [] for pred in cum_mean: scores.append(accuracy_score(y_val, np.argmax(pred, axis=1))) . That is it! Plot the resulting scores to obtain similar plot to one that appeared on the slides. . plt.figure(figsize=(10, 6)) plt.plot(scores, linewidth=3) plt.xlabel(&#39;num_trees&#39;) plt.ylabel(&#39;accuracy&#39;); . We see, that 150 trees are already sufficient to have stable result. .",
            "url": "https://aaabulkhair.github.io/Analixa/2022/02/05/best-n-estimators-for-randomforest.html",
            "relUrl": "/2022/02/05/best-n-estimators-for-randomforest.html",
            "date": " • Feb 5, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This blog is powered by Ahmed Abulkhair a Data Scientist and Teaching Assistant at Information Technology Institute, Data Science track, with a strong math background and +3 years of experience in predictive modeling, data processing, machine learning, and deep learning. Also, I have a very special interest to NLP, and GANs! .",
          "url": "https://aaabulkhair.github.io/Analixa/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://aaabulkhair.github.io/Analixa/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}