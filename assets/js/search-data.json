{
  
    
        "post0": {
            "title": "Text Preprocessing | NLP 1",
            "content": "Introduction . When dealing with text data with Machine Learning models. There&#39;s a huge and simple problem! Which is, . Algorithms can not comprehend text, only numbers! . So, is it all about converting text into numbers? Of course, not! The natural language is a very senstive and expressive type of data which means alot of expressions and desires can be hidden in a piece of text an numbers are less likely to demonestrate that! Numbers can reveal very little information when comparing to the natural text. so, it this set of tutorials, we will try to handle most of the preprocessing techniques. Strating from the very basic (like what we will do in this tutorial) ending up with very complex. . Setup . We will be using the Natural Language Toolkit (NLTK) package and SpaCy, open-source Python libraries for natural language processing. NLTK has modules for collecting, handling, and processing Twitter data. . For this tutorial, we will use a Twitter dataset that comes with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. Let us import them now as well as a few other libraries we will be using. . import nltk # Python library for NLP import spacy # Python libray for NLP from nltk.corpus import twitter_samples # sample Twitter dataset from NLTK import matplotlib.pyplot as plt # library for visualization import seaborn as sns # library for visualization import random # pseudo-random number generator . About the Twitter dataset . The sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial. . You can download the dataset in your workspace by doing: . nltk.download(&#39;twitter_samples&#39;) . [nltk_data] Downloading package twitter_samples to [nltk_data] /usr/share/nltk_data... [nltk_data] Package twitter_samples is already up-to-date! . True . We can load the text fields of the positive and negative tweets by using the module&#39;s strings() method like this: . all_positive_tweets = twitter_samples.strings(&#39;positive_tweets.json&#39;) all_negative_tweets = twitter_samples.strings(&#39;negative_tweets.json&#39;) . Next, we&#39;ll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets . print(&#39;Number of positive tweets: &#39;, len(all_positive_tweets)) print(&#39;Number of negative tweets: &#39;, len(all_negative_tweets)) print(&#39; nThe type of all_positive_tweets is: &#39;, type(all_positive_tweets)) print(&#39;The type of a tweet entry is: &#39;, type(all_negative_tweets[0])) . Number of positive tweets: 5000 Number of negative tweets: 5000 The type of all_positive_tweets is: &lt;class &#39;list&#39;&gt; The type of a tweet entry is: &lt;class &#39;str&#39;&gt; . Now, lets construct a pie chart to point out class distrbution. . colors = sns.color_palette(&#39;pastel&#39;)[0:5] #create pie chart plt.figure(figsize=(8,8)) plt.pie([len(all_positive_tweets), len(all_negative_tweets)], labels = [&#39;Positive&#39;, &#39;Negative&#39;], colors = colors, autopct=&#39;%.0f%%&#39;) plt.show() . Looking at raw texts . Before anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we&#39;d like to consider when preprocessing our data. . Below, you will print one random positive and one random negative tweet. We have added a color mark at the beginning of the string to further distinguish the two. (Warning: This is taken from a public dataset of real tweets and a very small portion has explicit content.) . print(&#39; 033[92m&#39; + all_positive_tweets[random.randint(0,5000)]) # print negative in red print(&#39; 033[91m&#39; + all_negative_tweets[random.randint(0,5000)]) . @PuppyShogun mistakes happen man, as long as we get to play the game, we&#39;ll be happy :) ＠maverickgamer_　July 24, 2015 at 07:17PM 　:( . One observation, is the presence of emoticons and URLs in many of the tweets. This info will come in handy in the next steps. . Steps of Preprocessing . Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks: . Removing unnesseary parts (mentions, tags, hashtags, and URLs) | Lowercasing | Removing stop words and punctuation | Tokenizing the string | Stemming and/or lemmetization | . Besides, the first purpose of preprocessing which is converting text into numbers, there&#39;s one more important purpose, which is about reducing the redundency of the data size itself. In other words, we want to represent the text features in the most minimal feature space and this will be demonestrated clearly for each step. . Let&#39;s import some libraries to help us out! . import re # library for regular expression operations import string # for string operations import spacy # Text processing . tweet = all_positive_tweets[2277] print(tweet) . My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i . Removing unnesseary parts (mentions, tags, hashtags, and URLs) . Since we have a Twitter dataset, we&#39;d like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We&#39;ll use the re library to perform regular expression operations on our tweet. We&#39;ll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. &#39;&#39;) . RT_remover = lambda x : re.sub(r&#39;^b s([RT]+)?&#39;,&#39;&#39;, x) # remove all URLs URL_remover = lambda x: re.sub(r&#39;http S+&#39;, &#39;&#39;, x) # remove hashtags # only removing the hash # sign from the word Hashtag_remover = lambda x: re.sub(r&#39;#&#39;, &#39;&#39;, x) # Apply all functions tweet = RT_remover(tweet) tweet = URL_remover(tweet) tweet = Hashtag_remover(tweet) # print final output print(tweet) . My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… . If you need any further info about using and testing regex, you may visit this website . Lowercasing . This step is often done for the sake of reducing the feature representation space. . For example: . I love cars! Cars are the most passionate hoppy in the world. | . Without lowercasing, cars and Cars with be two different words with two different representation which is not true nor optimal for sure. . lowercase = lambda x : x.lower() # Apply function tweet = lowercase(tweet) # Print result print(tweet) . my beautiful sunflowers on a sunny friday morning off :) sunflowers favourites happy friday off… . Tokenization . Although this word seems to be complex but it means a very simple process which is dividing the natural text into parts! This will make the process of generating a feature representation much easier. . Here, we will divide our natural text into words which are the basic building block of a text. We will be doing that using some built-in functions such as split() and strip() . def tokenize(text) -&gt; str: &quot;&quot;&quot; transform text into list of tokens - param: - text: input text -&gt; str - return - text_tokens: list of tokens &quot;&quot;&quot; text = text.strip() text_tokens = text.split() return text_tokens # test the function tweet_tokens = tokenize(tweet) tweet_tokens . [&#39;my&#39;, &#39;beautiful&#39;, &#39;sunflowers&#39;, &#39;on&#39;, &#39;a&#39;, &#39;sunny&#39;, &#39;friday&#39;, &#39;morning&#39;, &#39;off&#39;, &#39;:)&#39;, &#39;sunflowers&#39;, &#39;favourites&#39;, &#39;happy&#39;, &#39;friday&#39;, &#39;off…&#39;] . Removing stop words and punctuation . Stop words and punctuation don&#39;t give a lot of meaning in most cases. Let&#39;s see an example. . There is a ball on the table. It is huge and colorful! . Lets remove stop words and punctuation and see if we can capture the same meaning. . ball table huge colorful . As you can see, it&#39;s much shorter and this will defintly lead to lower feature representation. Some packages have provided lists for the stopwords and punctuation in many languges. In this tutorial, we will be using SpaCy and String packages. . en = spacy.load(&#39;en_core_web_sm&#39;) # Donwload Stopwords stopwords = en.Defaults.stop_words # Convert to list lst_stopwords = list(stopwords) print(&#39;Stop words n&#39;) print(lst_stopwords) print(&#39; nPunctuation n&#39;) print(string.punctuation) . Stop words [&#39;‘s&#39;, &#39;even&#39;, &#39;everything&#39;, &#39;how&#39;, &#39;thereafter&#39;, &#39;being&#39;, &#39;mine&#39;, &#39;up&#39;, &#39;make&#39;, &#39;really&#39;, &#39;’d&#39;, &#39;own&#39;, &#39;becoming&#39;, &#39;none&#39;, &#39;but&#39;, &#39;between&#39;, &#39;down&#39;, &#39;’s&#39;, &#39;whoever&#39;, &#39;been&#39;, &#39;every&#39;, &#39;else&#39;, &#39;’m&#39;, &#39;unless&#39;, &#39;serious&#39;, &#39;ever&#39;, &#39;those&#39;, &#39;above&#39;, &#39;wherever&#39;, &#39;cannot&#39;, &#39;any&#39;, &#39;back&#39;, &#39;here&#39;, &#39;whom&#39;, &#39;to&#39;, &#39;well&#39;, &#39;fifteen&#39;, &#39;each&#39;, &#39;due&#39;, &#39;eight&#39;, &#39;whenever&#39;, &#39;often&#39;, &#39;last&#39;, &#39;both&#39;, &#39;ca&#39;, &#39;throughout&#39;, &#39;go&#39;, &#39;mostly&#39;, &#39;three&#39;, &#39;before&#39;, &#39;or&#39;, &#39;few&#39;, &#39;for&#39;, &#39;such&#39;, &#39;’re&#39;, &#39;would&#39;, &#39;done&#39;, &#39;thru&#39;, &#39;almost&#39;, &#39;eleven&#39;, &#39;other&#39;, &#39;do&#39;, &#39;this&#39;, &#39;hereby&#39;, &#39;along&#39;, &#39;n‘t&#39;, &#34;&#39;d&#34;, &#39;next&#39;, &#39;one&#39;, &#39;seeming&#39;, &#39;were&#39;, &#39;then&#39;, &#39;yourself&#39;, &#39;part&#39;, &#39;wherein&#39;, &#39;while&#39;, &#39;anywhere&#39;, &#39;twenty&#39;, &#39;her&#39;, &#39;beforehand&#39;, &#39;put&#39;, &#39;somehow&#39;, &#39;off&#39;, &#39;either&#39;, &#39;very&#39;, &#39;us&#39;, &#39;perhaps&#39;, &#39;over&#39;, &#39;various&#39;, &#39;no&#39;, &#39;also&#39;, &#39;thus&#39;, &#39;via&#39;, &#39;ours&#39;, &#34;&#39;ll&#34;, &#39;someone&#39;, &#39;onto&#39;, &#39;ten&#39;, &#39;just&#39;, &#39;further&#39;, &#39;had&#39;, &#39;within&#39;, &#39;a&#39;, &#39;nine&#39;, &#39;him&#39;, &#39;his&#39;, &#39;never&#39;, &#39;third&#39;, &#39;full&#39;, &#39;nothing&#39;, &#39;hundred&#39;, &#39;there&#39;, &#39;hers&#39;, &#39;around&#39;, &#39;indeed&#39;, &#39;now&#39;, &#39;can&#39;, &#39;nevertheless&#39;, &#39;‘re&#39;, &#39;therein&#39;, &#39;seem&#39;, &#39;twelve&#39;, &#39;thereby&#39;, &#39;might&#39;, &#39;from&#39;, &#39;others&#39;, &#39;hereupon&#39;, &#39;an&#39;, &#39;again&#39;, &#39;made&#39;, &#39;once&#39;, &#39;though&#39;, &#39;five&#39;, &#39;by&#39;, &#39;‘ll&#39;, &#39;at&#39;, &#39;several&#39;, &#39;whither&#39;, &#39;their&#39;, &#39;our&#39;, &#39;least&#39;, &#39;some&#39;, &#39;too&#39;, &#39;the&#39;, &#39;they&#39;, &#39;except&#39;, &#39;who&#39;, &#39;thereupon&#39;, &#39;meanwhile&#39;, &#39;side&#39;, &#39;you&#39;, &#39;another&#39;, &#39;elsewhere&#39;, &#39;first&#39;, &#39;per&#39;, &#39;yet&#39;, &#39;formerly&#39;, &#39;‘m&#39;, &#39;whence&#39;, &#39;latterly&#39;, &#39;whether&#39;, &#39;used&#39;, &#39;across&#39;, &#39;he&#39;, &#39;enough&#39;, &#39;will&#39;, &#39;whole&#39;, &#34;n&#39;t&#34;, &#39;all&#39;, &#39;we&#39;, &#39;fifty&#39;, &#39;always&#39;, &#39;after&#39;, &#39;i&#39;, &#39;bottom&#39;, &#39;himself&#39;, &#39;n’t&#39;, &#39;afterwards&#39;, &#39;upon&#39;, &#39;with&#39;, &#39;ourselves&#39;, &#39;my&#39;, &#39;therefore&#39;, &#34;&#39;s&#34;, &#39;most&#39;, &#39;top&#39;, &#39;under&#39;, &#39;your&#39;, &#39;not&#39;, &#39;nowhere&#39;, &#39;anyway&#39;, &#39;whatever&#39;, &#39;anyone&#39;, &#39;‘ve&#39;, &#39;through&#39;, &#39;rather&#39;, &#39;whereafter&#39;, &#39;seemed&#39;, &#39;so&#39;, &#39;get&#39;, &#39;nor&#39;, &#39;against&#39;, &#39;moreover&#39;, &#39;yourselves&#39;, &#39;anyhow&#39;, &#39;about&#39;, &#39;move&#39;, &#39;was&#39;, &#39;yours&#39;, &#39;because&#39;, &#39;below&#39;, &#39;six&#39;, &#39;latter&#39;, &#39;everyone&#39;, &#39;noone&#39;, &#39;still&#39;, &#39;does&#39;, &#39;must&#39;, &#39;them&#39;, &#39;call&#39;, &#39;why&#39;, &#39;‘d&#39;, &#39;since&#39;, &#39;amount&#39;, &#39;alone&#39;, &#39;regarding&#39;, &#39;became&#39;, &#39;and&#39;, &#39;may&#39;, &#39;former&#39;, &#39;myself&#39;, &#39;doing&#39;, &#39;less&#39;, &#39;she&#39;, &#39;into&#39;, &#39;’ll&#39;, &#39;sometime&#39;, &#39;although&#39;, &#39;name&#39;, &#39;itself&#39;, &#39;herself&#39;, &#39;besides&#39;, &#39;it&#39;, &#39;as&#39;, &#39;when&#39;, &#39;thence&#39;, &#39;if&#39;, &#39;front&#39;, &#39;seems&#39;, &#39;whereas&#39;, &#39;give&#39;, &#39;these&#39;, &#39;empty&#39;, &#39;themselves&#39;, &#39;many&#39;, &#39;see&#39;, &#39;much&#39;, &#39;become&#39;, &#39;only&#39;, &#39;forty&#39;, &#39;could&#39;, &#39;that&#39;, &#39;what&#39;, &#39;otherwise&#39;, &#39;re&#39;, &#39;on&#39;, &#39;say&#39;, &#39;sometimes&#39;, &#39;amongst&#39;, &#39;in&#39;, &#39;namely&#39;, &#39;together&#39;, &#39;please&#39;, &#39;which&#39;, &#39;towards&#39;, &#39;without&#39;, &#39;hereafter&#39;, &#39;during&#39;, &#39;everywhere&#39;, &#39;has&#39;, &#39;out&#39;, &#39;whose&#39;, &#34;&#39;re&#34;, &#39;more&#39;, &#34;&#39;ve&#34;, &#39;am&#39;, &#39;whereby&#39;, &#39;nobody&#39;, &#39;beside&#39;, &#39;somewhere&#39;, &#39;quite&#39;, &#39;toward&#39;, &#39;me&#39;, &#39;’ve&#39;, &#39;herein&#39;, &#39;using&#39;, &#39;beyond&#39;, &#39;have&#39;, &#39;among&#39;, &#39;than&#39;, &#34;&#39;m&#34;, &#39;where&#39;, &#39;did&#39;, &#39;until&#39;, &#39;sixty&#39;, &#39;whereupon&#39;, &#39;are&#39;, &#39;its&#39;, &#39;however&#39;, &#39;becomes&#39;, &#39;behind&#39;, &#39;something&#39;, &#39;neither&#39;, &#39;same&#39;, &#39;already&#39;, &#39;four&#39;, &#39;two&#39;, &#39;is&#39;, &#39;show&#39;, &#39;keep&#39;, &#39;of&#39;, &#39;anything&#39;, &#39;should&#39;, &#39;hence&#39;, &#39;be&#39;, &#39;take&#39;] Punctuation !&#34;#$%&amp;&#39;()*+,-./:;&lt;=&gt;?@[ ]^_`{|}~ . tweets_clean = list() # clean all word tokens for wrd in tweet_tokens: # Iterate over all words if (wrd not in lst_stopwords and wrd not in set(string.punctuation)): tweets_clean.append(wrd) # Add to clean tweets list tweets_clean . [&#39;beautiful&#39;, &#39;sunflowers&#39;, &#39;sunny&#39;, &#39;friday&#39;, &#39;morning&#39;, &#39;:)&#39;, &#39;sunflowers&#39;, &#39;favourites&#39;, &#39;happy&#39;, &#39;friday&#39;, &#39;off…&#39;] . Stemming and/or lemmetization . Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary. . Consider the words: . learn | learning | learned | learnt | . All these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, happi and sunni. That&#39;s because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy: . happy | happiness | happier | . We can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen. . Lemmetization is the process of coverting a word into the least meaningfull part of it. Unlike stemming, lemmetization has to produce the least correct root with correct spelling. . Word Lemma . happiness | happiness | . happier | happy | . happier | happy | . Now, lets see some words that might make you feel the differnce. . Word Stem Lemma . caring | care | caring | . ties | ti | tie | . easily | easili | easily | . mice | mice | mouse | . As you can see, stemming can sometimes produce uncorrect words such as ti and easili. On contrary, the lemmetization always produce a correct word in spelling even if this will lead to higher feature representation. For example, caring and care will be to different elements in the feature representation, however they share the same root car when dealing with stemming instead. We will be using NLTK library for that purpose for lemmetizing this data. . from nltk.stem import WordNetLemmatizer # Intiate WordNetLemmatizer lemmatizer = WordNetLemmatizer() # Lemmetize the words tweets_lemmas = [] for wrd in tweets_clean: wrd = lemmatizer.lemmatize(wrd) tweets_lemmas.append(wrd) tweets_lemmas . [&#39;beautiful&#39;, &#39;sunflower&#39;, &#39;sunny&#39;, &#39;friday&#39;, &#39;morning&#39;, &#39;:)&#39;, &#39;sunflower&#39;, &#39;favourite&#39;, &#39;happy&#39;, &#39;friday&#39;, &#39;off…&#39;] . Putting it all together. . By now, we have completed all the required preprocessing steps. Lets combine them all in a one preprocess function that will be used in our next tutorial. . def preprocess(txt) -&gt; str: # remove old style retweet text &quot;RT&quot; RT_remover = lambda x : re.sub(r&#39;^b s([RT]+)?&#39;,&#39;&#39;, x) # remove all URLs URL_remover = lambda x: re.sub(r&#39;http S+&#39;, &#39;&#39;, x) # remove hashtags # only removing the hash # sign from the word Hashtag_remover = lambda x: re.sub(r&#39;#&#39;, &#39;&#39;, x) # Apply all functions txt = RT_remover(txt) txt = URL_remover(txt) txt = Hashtag_remover(txt) def tokenize(text) -&gt; str: &quot;&quot;&quot; transform text into list of tokens - param: - text: input text -&gt; str - return - text_tokens: list of tokens &quot;&quot;&quot; text = text.strip() text_tokens = text.split() return text_tokens lst_txt = tokenize(txt) # Import modules import string import spacy from nltk.stem import WordNetLemmatizer # Load the core utils for english languge en = spacy.load(&#39;en_core_web_sm&#39;) # Donwload Stopwords stopwords = en.Defaults.stop_words # Convert to list lst_stopwords = list(stopwords) lst_txt_clean = list() # clean all word tokens for wrd in lst_txt: # Iterate over all words if (wrd not in lst_stopwords and wrd not in set(string.punctuation)): lst_txt_clean.append(wrd) # Add to clean tweets list lemmatizer = WordNetLemmatizer() # Lemmetize the words lst_txt_lemmas = [] for wrd in lst_txt_clean: wrd = lemmatizer.lemmatize(wrd) lst_txt_lemmas.append(wrd) return lst_txt_lemmas . Let&#39;s test it! . sample_tweet = all_positive_tweets[random.randint(0,5000)] preprocess(sample_tweet) . [&#39;Well&#39;, &#39;morning&#39;, &#39;carry&#39;, &#39;great.&#39;, &#39;Work&#39;, &#39;:)&#39;] . That is the end of this tutorial! If you find this beneficial. Follow me for more updates! . LinkedIn | Kaggle | .",
            "url": "https://aaabulkhair.github.io/Analixa/machine%20learning/nlp/nltk/2022/02/07/nlp-text-preprocessing-1.html",
            "relUrl": "/machine%20learning/nlp/nltk/2022/02/07/nlp-text-preprocessing-1.html",
            "date": " • Feb 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Titanic | The Power of Sklearn",
            "content": "Introduction . Neither Titanic dataset nor sklearn a new thing for any data scientist but there are some important features in scikit-learn that will make any model preprocessing and tuning easier, to be specific this notebook will cover the following concepts: . ColumnTransformer | Pipeline | SimpleImputer | StandardScalar | OneHotEncoder | OrdinalEncoder | GridSearch | . Note, this tutorial is a solution to the famous kaggle competition Titanic - Machine Learning from Disaster . Mounting Filesystem . import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # Any results you write to the current directory are saved as output. . /kaggle/input/titanic/gender_submission.csv /kaggle/input/titanic/test.csv /kaggle/input/titanic/train.csv . Import Packages . import pandas as pd # Numpy for Numerical operations import numpy as np # Import ColumnTransformer from sklearn.compose import ColumnTransformer # Import Pipeline from sklearn.pipeline import Pipeline # Import SimpleImputer from sklearn.impute import SimpleImputer # Import StandardScaler, OneHotEncodr and OrdinalEncoder from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder # Import Random Forest for Classification from sklearn.ensemble import RandomForestClassifier # Import GridSearch from sklearn.model_selection import GridSearchCV . Reading Data . In the following cells, we will read the train and test data and check for NaNs. . train_data = pd.read_csv(&quot;/kaggle/input/titanic/train.csv&quot;) # See some info train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): PassengerId 891 non-null int64 Survived 891 non-null int64 Pclass 891 non-null int64 Name 891 non-null object Sex 891 non-null object Age 714 non-null float64 SibSp 891 non-null int64 Parch 891 non-null int64 Ticket 891 non-null object Fare 891 non-null float64 Cabin 204 non-null object Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB . It&#39;s obvious that we had to deal with NaNs . test_data = pd.read_csv(&quot;/kaggle/input/titanic/test.csv&quot;) test_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): PassengerId 418 non-null int64 Pclass 418 non-null int64 Name 418 non-null object Sex 418 non-null object Age 332 non-null float64 SibSp 418 non-null int64 Parch 418 non-null int64 Ticket 418 non-null object Fare 417 non-null float64 Cabin 91 non-null object Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB . Splitting Data . X_train = train_data.drop([&#39;Survived&#39;, &#39;Name&#39;], axis = 1) X_test = test_data.drop([&#39;Name&#39;], axis = 1) y_train = train_data[&#39;Survived&#39;] . Continuous and Numerical features handling . It&#39;s clear that we have some numerical features that have some missing values to be imputed and they have to be of the same scale also. . In the following cell, we will handle the numerical features separtely i.e &quot;Age&quot; and &quot;Fare&quot; . # Difine a list with the numeric features numeric_features = [&#39;Age&#39;, &#39;Fare&#39;] # Define a pipeline for numer&quot;ic features numeric_features_pipeline = Pipeline(steps= [ (&#39;imputer&#39;, SimpleImputer(strategy = &#39;median&#39;)), # Impute with median value for missing (&#39;scaler&#39;, StandardScaler()) # Conduct a scaling step ]) . Categorical features handling . It&#39;s clear that we have some categorical features that have some missing values to be imputed and they have to be encoded using one hot encoding. . In the following cell, we will handle the categorical features separtely i.e &quot;Embarked&quot; and &quot;Sex&quot; . Note: I choose simple imputer for the missing cells to impute with &#39;missing&#39; word. My aim was to gather all missing cells in one category for further encoding. . # Difine a list with the categorical features categorical_features = [&#39;Embarked&#39;, &#39;Sex&#39;] # Define a pipeline for categorical features categorical_features_pipeline = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value = &#39;missing&#39;)), # Impute with the word &#39;missing&#39; for missing values (&#39;onehot&#39;, OneHotEncoder(handle_unknown = &#39;ignore&#39;)) # Convert all categorical variables to one hot encoding ]) . Ordinal features handling . Passenger class or &#39;Pclass&#39; for short is an ordinal feature that must be handled keeping in mind that class 3 is much higher than 2 and so on. . # Define a list with the ordinal features ordinal_features = [&#39;Pclass&#39;] # Define a pipline for ordinal features ordinal_features_pipeline = Pipeline(steps=[ (&#39;ordinal&#39;, OrdinalEncoder(categories= [[1, 2, 3]])) ]) . Construct a comprehended preprocessor . Now, we will create a preprocessor that can handle all columns in our dataset using ColumnTransformer . preprocessor = ColumnTransformer(transformers= [ # transformer with name &#39;num&#39; that will apply # &#39;numeric_features_pipeline&#39; to numeric_features (&#39;num&#39;, numeric_features_pipeline, numeric_features), # transformer with name &#39;cat&#39; that will apply # &#39;categorical_features_pipeline&#39; to categorical_features (&#39;cat&#39;, categorical_features_pipeline, categorical_features), # transformer with name &#39;ord&#39; that will apply # &#39;ordinal_features_pipeline&#39; to ordinal_features (&#39;ord&#39;, ordinal_features_pipeline, ordinal_features) ]) . Prediction Pipeline . Now, we will create a full prediction pipeline that uses our preprocessor and then transfer it to our classifier of choice &#39;Random Forest&#39;. . clf = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;classifier&#39;, RandomForestClassifier(n_estimators = 120, max_leaf_nodes = 100))]) . Pipeline Training . Let&#39;s train our pipeline now . clf.fit(X_train, y_train) . Pipeline(memory=None, steps=[(&#39;preprocessor&#39;, ColumnTransformer(n_jobs=None, remainder=&#39;drop&#39;, sparse_threshold=0.3, transformer_weights=None, transformers=[(&#39;num&#39;, Pipeline(memory=None, steps=[(&#39;imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy=&#39;median&#39;, verbose=0)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean... RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=100, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=120, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False))], verbose=False) . Pipeline Tuning . The question now, can we push it a little bit further? i.e. can we tune every single part or our Pipeline? . Here, I will use GridSearch to decide three things: . Simple Imputer strategy :mean or median&gt; - n_estimators of Random Forest | max leaf nodes of Random Forest | . Note, you can access any parameter from the outer level to the next adjacent inner one . For Example:to access the strategy of the Simple Imputer you can do the followingpreprocessornumimputer__strategy . Let&#39;s see this into action . param_grid = { &#39;preprocessor__num__imputer__strategy&#39;: [&#39;mean&#39;, &#39;median&#39;], &#39;classifier__n_estimators&#39;: [100, 120, 150, 170, 200], &#39;classifier__max_leaf_nodes&#39; : [100, 120, 150, 170, 200] } grid_search = GridSearchCV(clf, param_grid, cv=10) grid_search.fit(X_train, y_train) print((&quot;best random forest from grid search: %.3f&quot; % grid_search.score(X_train, y_train))) print(&#39;The best parameters of Simple Imputer and C are:&#39;) print(grid_search.best_params_) . best random forest from grid search: 0.944 The best parameters of Simple Imputer and C are: {&#39;classifier__max_leaf_nodes&#39;: 100, &#39;classifier__n_estimators&#39;: 150, &#39;preprocessor__num__imputer__strategy&#39;: &#39;median&#39;} . Generate Predictions . Let&#39;s generate predictions now using our grid search model and submit the results . predictions = grid_search.predict(X_test) # Generate results dataframe results_df = pd.DataFrame({&#39;PassengerId&#39;: test_data.PassengerId, &#39;Survived&#39;: predictions}) # Save to csv file results_df.to_csv(&#39;submission.csv&#39;, index = False) print(&#39;Submission CSV has been saved!&#39;) . Submission CSV has been saved! .",
            "url": "https://aaabulkhair.github.io/Analixa/machine%20learning/sklearn/python/2022/02/05/titanic-power-of-scikit-learn.html",
            "relUrl": "/machine%20learning/sklearn/python/2022/02/05/titanic-power-of-scikit-learn.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Choosing Best n_estimators for RandomForest model without retraining",
            "content": "In this notebook, we will try to determine the best number of n_estimators for RandomForest model without training the model for multiple times . Load Dataset . We will use one of the built-in datasets, which is digits . import sklearn.datasets from sklearn.model_selection import train_test_split # Load dataset X, y = sklearn.datasets.load_digits(n_class = 10,return_X_y = True) # Split the data X_train, X_val, y_train, y_val = train_test_split(X, y) . Import libraries . import numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score . Step 1: first fit a Random Forest to the data. Set n_estimators to a high value. . rf = RandomForestClassifier(n_estimators=500, max_depth=4, n_jobs=-1) rf.fit(X_train, y_train) . RandomForestClassifier(max_depth=4, n_estimators=500, n_jobs=-1) . Step 2: Get predictions for each tree in Random Forest separately. . predictions = [] for tree in rf.estimators_: predictions.append(tree.predict_proba(X_val)[None, :]) . Step 3: Concatenate the predictions to a tensor of size (number of trees, number of objects, number of classes). . predictions = np.vstack(predictions) . Step 4: Сompute cumulative average of the predictions. That will be a tensor, that will contain predictions of the random forests for each n_estimators. . cum_mean = np.cumsum(predictions, axis=0)/np.arange(1, predictions.shape[0] + 1)[:, None, None] . Step 5: Get accuracy scores for each n_estimators value . scores = [] for pred in cum_mean: scores.append(accuracy_score(y_val, np.argmax(pred, axis=1))) . That is it! Plot the resulting scores to obtain similar plot to one that appeared on the slides. . plt.figure(figsize=(10, 6)) plt.plot(scores, linewidth=3) plt.xlabel(&#39;num_trees&#39;) plt.ylabel(&#39;accuracy&#39;); . We see, that 150 trees are already sufficient to have stable result. .",
            "url": "https://aaabulkhair.github.io/Analixa/machine%20learning/randomforest/classification/python/2022/02/05/best-n-estimators-for-randomforest.html",
            "relUrl": "/machine%20learning/randomforest/classification/python/2022/02/05/best-n-estimators-for-randomforest.html",
            "date": " • Feb 5, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This blog is powered by Ahmed Abulkhair a Data Scientist and Teaching Assistant at Information Technology Institute, Data Science track, with a strong math background and +3 years of experience in predictive modeling, data processing, machine learning, and deep learning. Also, I have a very special interest to NLP, and GANs! .",
          "url": "https://aaabulkhair.github.io/Analixa/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://aaabulkhair.github.io/Analixa/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}