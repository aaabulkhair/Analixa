{
  
    
        "post0": {
            "title": "So, which ML Algorithm to use?!",
            "content": "A lot of data science practitioners found the process of selecting a machine learning algorithm overwhelming and confusing. That’s because there are a bunch of algorithms that can do the same task. For example, classification can be done using a Decision Tree, SVM, Logistic Regression, Naive Bayes, KNN, and Neural Network. . Now, which one should be used? To clarify a bit of the ambiguity related to that question, let’s answer a simpler one. . What is a machine learning algorithm trying to do?! . Any algorithm tries to translate a group of features into a useful prediction through some mathematical system that differs from one algorithm to another. So this translation process will vary also. . By now, you may have decided what keyword plays a major role in selecting an algorithm. It is simply nothing but the features themselves, and we are just trying to choose the best algorithm (translator) for our features. . In the next section, I will try to categorize the most widely known algorithms (translators) based on their behavior. . Decision Boundary Concept . . One of the concepts that must be very bright and clear in every data scientist’s mind is the decision boundary concept. Decision Boundary is what defines the algorithm behavior and how it sees the data and deals with it. In other words and metaphorically speaking, it tells us the strong points of each of our algorithms (translators). . This useful graph below is the best in illustrating this concept. Check from sklearn documentation for reproducibility. It shows the performance of different algorithms for different data sets. . . Take a moment to grasp this graph because it reveals a lot about every algorithm. For example: . Nearest Neighbour Algorithms (KNN) is heavily relying on the closeness of points. | Linear SVM is trying to slice the data to decide the class of each data point. | RBF SVM is like KNN in finding linear combinations that separate the data but not in the way KNN does. | Decision Tree is tackling the problem differently. It is just to do some splits in the data to separate between the classes. | RandomForest is adopting the same strategy of Decision Tree but with more splits. | Neural Networks are also trying to get a linear combination of the data to separate classes. | AdaBoost is also adopting the idea of splits but in an enhanced and modified way. | . Based on these observations, let’s try to categorize our algorithms now. . Linear Models . The term linear model implies that the model is specified as a linear combination of features. Based on training data, the learning process computes one weight for each feature to form a model that can predict or estimate the target value. . . This category includes the following algorithms: . Linear Regression | Logistic Regression | SVM | Neural Networks | . Tree-based Models . Tree-based models use a series of if-then rules to generate predictions from one or more decision trees. This is also what causes this splitting effect that you can easily see in the above graph. . . This category includes the following algorithms: . Decision Tree | Random Forest (ensemble methods) | XGBoost (ensemble methods) | LightGBM (ensemble methods) | GradientBoosting (ensemble methods) | AdaBoost (ensemble methods) | CatBoost (ensemble methods) | . Distance-based Models . Distance-based models rely on determining the decision boundary based on the closeness of the points to each other. So, they are profoundly affected by the scale of each feature. . . Based on the previous categorization, we can turn our question to a simpler one. Are the features in our data helpful for the idea of splits, so we should pick one of the tree-based models? Or are they more useful in creating linear trends between the features and the target so we should choose a linear model? . Now, what should we do to answer these questions? . . **Exploratory Data Analysis (EDA)** . The ultimate goal of the EDA process is to know, explore, and visualize your data. Also, after understanding the data, EDA should help in deciding the best algorithm for your data. Maybe EDA’s process has no clear steps to do, but we can do a little summarization of this process. Besides, we will comment on how each step can reveal a piece of info about the best algorithm to use. . 1- Look at Summary statistics and visualizations . Percentiles, ranges, variance, and standard deviation can help identify the range for most of the data. | Averages and medians can describe the central tendency. | Correlations can indicate strong relationships. | . 2- Visualize the data . Box plots can identify outliers. | Density plots and histograms show the spread of data. | Scatter plots can describe bivariate relationships. | . Now, let’s comment about how the outcomes of these two steps will contribute to model selection. . Outliers . Lots of the above statistics and measures will denote information about the dispersion of our data. Keep in mind that lots of outliers will affect any linear model you choose, as the model will try to fit the points with high weights. This should make you think about the following questions. – Would a linear model help with the existence of these outliers? – If the outliers’ problem persists, What is the best way to handle them, and what is the handling method that will serve our model of choice? . Normality . The above measures and graphs show the distribution of our data and the correlation between features and the target variable. This should make you think about another two critical things . Lack of data normality may add a point in using a tree-based model as the idea of splits is less affected by the data normality. . | The strong correlation will add a point in using a linear model as it makes it easier for a model to construct some sort of linear combination boundaries. On the other hand, using a tree-based model will be less affected by the weak correlation. . | . Missing Values . The information denoted about the missing values should ignite some thoughts in every data scientist’s mind. For example, . Are the missing values related to some specific event, so they should be treated as a separate category of data? Consequently, they will be more beneficial to a tree-based model? | If the need for a linear model persists, what is the best imputation technique that will make these values of a considerable effect on our model? | Will using a model that can handle the missing values internally like Naive Bayes or XGBoost alleviate the problem? (Note: sklearn implementation for Naive Bayes algorithm does not allow missing values, but you can implement it manually) | . Feature Engineering . After the process of EDA, you must have thoughts about the model of choice. At least you are more inclined to model family over another. Now, the process of feature engineering should be done with respect to some model family. Let’s see some of the standard procedures in feature engineering and how they affect the model performance. . Missing Values Handling . Imputing the missing values with the “Unknown” category when dealing with categorical variables will be more beneficial to tree-based models. | Imputing the missing values with the mean or median will be more beneficial to linear models over tree-based models. | Models that can handle missing values can be susceptible, and not handled missing values can lead them to more reduced performance. | . Outliers Handling . Clipping can be with more value to any linear model as it increases the data normality, but it causes some information loss. | Transformations like logarithmic or square root transformation can add a damping effect to the values without any information loss. So, it’s more beneficial to linear models. | Tree-based models are less affected by outliers in general because the idea of splits in them will most probably assign them to a separate split. | . Scaling and normalization . Feature Scaling or normalization has no value when it comes to the tree-based models. That’s because the idea of splits will not be affected by whatever the scale of the data is. | Normalization is an excellent approach when dealing with linear models. It will lead to faster training and better scores. | Distance-based models are profoundly affected by the scale of the features. This will open up a door to enforce one feature over another just by increasing its scale. | . Categorical Variables Handling . Some approaches, like One-hot encoding and frequency encoding, can be of great help to a linear model. | On reversal, approaches like label encoding are beneficial to a tree-based model as they will boost the ability of a model to do data splits. | . After sharing some thoughts in the process of EDA and feature engineering, that will make us more oriented to the best model that can fit our problem. Let’s talk more about non-data related considerations that may affect our choice of the model. . Deployment Considerations . In the next few lines, we will ask questions related to our model of choice but not the data itself. For example: . What is your data storage capacity? Depending on your system’s storage capacity, you might not be able to store gigabytes of classification/regression models or gigabytes of data to train on. | Does the prediction have to be fast? In real-time applications, it is essential to have a prediction as quickly as possible. For instance, in autonomous driving, road signs must be classified as fast as possible to avoid accidents. | Does the learning and training process have to be fast? In some circumstances, training models quickly is necessary: sometimes, you need to rapidly update, on the fly, your model with a different dataset. | . Usability Considerations . Now, as we have discussed various families of models, best practices in EDA and feature engineering should be done with respect to a model family, and addressed some deployment considerations that may outweigh the use of one model over another. We will now discuss a crucial aspect of any model that every data scientist should be aware of. . Explainability Vs. Predictability . Explainability means how much you could explain your model prediction. For example, Decision Tree is a very explainable model. Once you have a prediction from it, you can quickly tell you why you get this prediction by following the series of splits (if-then rules). These models are often called white-box models. | Predictability means what is the predictive power of your algorithm, regardless of the ability to know why it gives a specific prediction for some input. For example, Neural Network is very complex to understand why it provides a particular prediction. These models are often called black-box models. | . Sometimes you must be aware of this trade-off as the application may require some explainability in your models of choice, so you will end up using simpler models or on reversal. You may not need any explainability for your application. Hence, you may prefer a model with high predictability over a simpler one. Below is a graph that shows this trade-off for various machine learning algorithms. . .",
            "url": "https://aaabulkhair.github.io/Analixa/machine%20learning/2022/05/17/So,-Which-ML-Algorithm-to-use!.html",
            "relUrl": "/machine%20learning/2022/05/17/So,-Which-ML-Algorithm-to-use!.html",
            "date": " • May 17, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Generative Adversarial Networks (GANs)",
            "content": "Introduction . Generative Adversarial Networks (GANs) | How GANs Work | GANs Process | Examples | Generating Hand-Written digits | . Generative Adversarial Networks (GANs) . Generative Adversarial Networks are used to generate images that never existed before. They learn about the world (objects, animals and so forth) and create new versions of those images that never existed. . They have two components: . A Generator - this creates the images. | A Discriminator - this assesses the images and tells the generator if they are similar to what it has been trained on. These are based off real world examples. | . When training the network, both the generator and discriminator start from scratch and learn together. . How GANs Work . G for Generative - this is a model that takes an input as a random noise singal and then outputs an image. . A for Adversarial - this is the discriminator, the opponent of the generator. This is capable of learning about objects, animals or other features specified. For example: if you supply it with pictures of dogs and non-dogs, it would be able to identify the difference between the two. . Using this example, once the discriminator has been trained, showing the discriminator a picture that isn&#39;t a dog it will return a 0. Whereas, if you show it a dog it will return a 1. . N for Network - meaning the generator and discriminator are both neural networks. . GANs Process . Step 1 - we input a random noise signal into the generator. The generator creates some images which is used for training the discriminator. We provide the discriminator with some features/images we want it to learn and the discriminator outputs probabilities. These probabilities can be rather high as the discriminator has only just started being trained. The values are then assessed and identified. The error is calculated and these are backpropagated through the discriminator, where the weights are updated. . Next we train the generator. We take the batch of images that it created and put them through the discriminator again. We do not include the feature images. The generator learns by tricking the discriminator into it outputting false positives. . The discriminator will provide an output of probabilities. The values are then assessed and compared to what they should have been. The error is calculated and backpropagated through the generator and the weights are updated. . Step 2 - This is the same as step 1 but the generator and discriminator are trained a little more. Through backpropagation the generator understands its mistakes and starts to make them more like the feature. . This is created through a Deconvolutional Neural Network. . Examples . GANs can be used for the following: . Generating Images | Image Modification | Super Resolution | Assisting Artists | Photo-Realistic Images | Speech Generation | Face Ageing | . It&#8217;s Training Cats and Dogs: NVIDIA Research Uses AI to Turn Cats Into Dogs, Lions and Tigers, Too . . Generating Hand-Written digits . Let&#39;s strat by importing some useful packages . import torch from torch import nn from tqdm.auto import tqdm from torchvision import transforms from torchvision.datasets import MNIST # Training dataset from torchvision.utils import make_grid from torch.utils.data import DataLoader import matplotlib.pyplot as plt torch.manual_seed(0) # Fixing Seed for Reproducibility import warnings warnings.filterwarnings(&#39;ignore&#39;) . Let&#39;s create a visualizer function to see the input and output data. . def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)): &#39;&#39;&#39; Function for visualizing images: Given a tensor of images, number of images, and size per image, plots and prints the images in a uniform grid. &#39;&#39;&#39; image_unflat = image_tensor.detach().cpu().view(-1, *size) image_grid = make_grid(image_unflat[:num_images], nrow=5) plt.imshow(image_grid.permute(1, 2, 0).squeeze()) plt.show() . MNIST Dataset . The training images your discriminator will be using is from a dataset called MNIST. It contains 60,000 images of handwritten digits, from 0 to 9, like these: . . Discriminator vs. Generator . The first thing that we will get to explore is the difference between a discriminator and a generator. Remember that these are the two main components of a GAN! . What is a discriminator? . One of the most widely used type of ML models is the classifier, which is used to sift through items in a dataset and classify them into different categories. For example, you might be familiar with image classifiers that discriminate between images of a cat and images of a dog. For a well-trained classifier: when you give it an image of a cat, it will say &quot;cat&quot;! When you give it an image of a dog, it will say &quot;dog&quot;! You can also use other classes, like coconuts vs. starfruit. . . Image Credit:Google In terms of probabilities, the classifier wants to find $p(y|x)$: the probability that given an image input $x$, the image class $y$ is cat, $p(y= text{cat}|x)$, or dog, $p(y= text{dog}|x)$. . The discriminator is simply a classifier with two classes: real and fake. Given an input x, the discriminator will calculate the probabilities $p(y= text{real}|x)$ and $p(y= text{fake}|x)$ and classify $x$. The input $x$ can be anything that you have the generator create and is not limited to images. Your GAN can be trained on videos, text, audio, etc. . What is a generator? . Generators are designed to have a different goal from discriminators (classifiers). Imagine you&#39;re working at a tropical fruit stand and asked to sort the fruit into two categories: coconuts and starfruit. That&#39;s the job of a classifier. But what if a customer comes up to the stand, and asks: what is a starfruit? You can&#39;t just say that it&#39;s not a coconut. You would need to explain what makes something a starfruit and what doesn&#39;t, not just its differences from a coconut. That&#39;s the job of a generator: to represent different classes in general, not just distinguish them. . In terms of probabilities, the generator wants to figure out $p(x|y)$: the probability that, given that you generated a starfruit $(y= text{starfruit})$, the resulting image $(x)$ is the one generated. The output space of possible starfruit images is huge, so that makes this challenging for the generator. . This can be a much harder task than discrimination. Typically, you will need the generator to take multiple steps to improve itself for every step the discriminator takes. It&#39;s easy to tell the difference between a coconut and a starfruit when you look at a mix of them, but to know exactly all the features of all possible coconuts in the world? That&#39;s a lot, but it&#39;s really cool if you can get even close to it, because you can start generating all sorts of coconuts and starfruit when you do. I don&#39;t have a generator for these tropical fruits figured out for you to play with, but you can generate all kinds of cool things with these models, like realistic faces! . In the below image, the generator is trying to find the features that represent all cats using the feedback from the discriminator. . . Image Credit:TensorFlow . Generator . The first step is to build the generator component. . We will start by creating a function to make a single layer/block for the generator&#39;s neural network. Each block should include a linear transformation to map to another shape, a batch normalization for stabilization, and finally a non-linear activation function (ReLU) so the output can be transformed in complex ways. . def generator_block(input_dim, output_dim): &#39;&#39;&#39; Function for returning a block of the generator&#39;s neural network given input and output dimensions. Parameters: input_dim: the dimension of the input vector, a scalar output_dim: the dimension of the output vector, a scalar Returns: a generator neural network layer, with a linear transformation followed by a batch normalization and then a relu activation &#39;&#39;&#39; return nn.Sequential( # Linear Transformation Layer nn.Linear(input_dim, output_dim), # BatchNorm1d for stablizing the training process nn.BatchNorm1d(output_dim), # Add some non-linearity effect nn.ReLU(inplace=True) ) . Now, we can build the generator class. It will take 3 values: . The noise vector dimension | The image dimension | The initial hidden dimension | . Using these values, the generator will build a neural network with 5 layers/blocks. Beginning with the noise vector, the generator will apply non-linear transformations via the block function until the tensor is mapped to the size of the image to be outputted (the same size as the real images from MNIST). The final layer does not need a normalization or activation function, but does need to be scaled with a sigmoid function. . Finally, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network. . class Generator(nn.Module): &#39;&#39;&#39; Generator Class Values: z_dim: the dimension of the noise vector, a scalar im_dim: the dimension of the images, fitted for the dataset used, a scalar (MNIST images are 28 x 28 = 784 so that is your default) hidden_dim: the inner dimension, a scalar &#39;&#39;&#39; def __init__(self, z_dim=10, im_dim=784, hidden_dim=128): super(Generator, self).__init__() # Building a Generator Neural Network # 6 - Generator Blocks (Previously Built) self.gen = nn.Sequential( generator_block(z_dim, hidden_dim), generator_block(hidden_dim, hidden_dim * 2), generator_block(hidden_dim * 2, hidden_dim * 4), generator_block(hidden_dim * 4, hidden_dim * 8), generator_block(hidden_dim * 8, hidden_dim * 16), generator_block(hidden_dim * 16, hidden_dim * 8), # Ouptput Layer = Linear Transformation + Sigmoid activation nn.Linear(hidden_dim * 8, im_dim), nn.Sigmoid() ) def forward(self, noise): &#39;&#39;&#39; Function for completing a forward pass of the generator: Given a noise tensor, returns generated images. Parameters: noise: a noise tensor with dimensions (n_samples, z_dim) &#39;&#39;&#39; return self.gen(noise) . Noise . In order to use your generator, you will need to be able to create noise vectors. The noise vector z has the important role of making sure the images generated from the same class don&#39;t all look the same -- It&#39;s like a random seed. We will generate it randomly using PyTorch by sampling random numbers from the normal distribution. Since multiple images will be processed per pass, we will generate all the noise vectors at once. . def get_noise(n_samples, z_dim, device=&#39;cuda&#39;): &#39;&#39;&#39; Function for creating noise vectors: Given the dimensions (n_samples, z_dim), creates a tensor of that shape filled with random numbers from the normal distribution. Parameters: n_samples: the number of samples to generate, a scalar z_dim: the dimension of the noise vector, a scalar device: the device type &#39;&#39;&#39; return torch.randn((n_samples, z_dim), device=device) . Discriminator . The second component that you need to construct is the discriminator. As with the generator component, We will start by creating a function that builds a neural network block for the discriminator. . Note: We use leaky ReLUs to prevent the &quot;dying ReLU&quot; problem. . def discriminator_block(input_dim, output_dim): &#39;&#39;&#39; Discriminator Block Function for returning a neural network of the discriminator given input and output dimensions. Parameters: input_dim: the dimension of the input vector, a scalar output_dim: the dimension of the output vector, a scalar Returns: a discriminator neural network layer, with a linear transformation followed by an nn.LeakyReLU activation with negative slope of 0.2 (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html) &#39;&#39;&#39; return nn.Sequential( nn.Linear(input_dim, output_dim), nn.LeakyReLU(negative_slope=0.2, inplace=True) ) . Now we can use these blocks to make a discriminator! The discriminator class holds 2 values: . The image dimension | The hidden dimension | . The discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator&#39;s neural network you are given a forward pass function that takes in an image tensor to be classified. . class Discriminator(nn.Module): &#39;&#39;&#39; Discriminator Class Values: im_dim: the dimension of the images, fitted for the dataset used, a scalar (MNIST images are 28x28 = 784 so that is your default) hidden_dim: the inner dimension, a scalar &#39;&#39;&#39; def __init__(self, im_dim=784, hidden_dim=128): super(Discriminator, self).__init__() self.disc = nn.Sequential( # 3 - Discriminator Blocks discriminator_block(im_dim, hidden_dim * 4), discriminator_block(hidden_dim * 4, hidden_dim * 8), discriminator_block(hidden_dim * 8, hidden_dim * 4), discriminator_block(hidden_dim * 4, hidden_dim * 2), discriminator_block(hidden_dim * 2, hidden_dim), # Adding a linear output nn.Linear(hidden_dim , 1) ) def forward(self, image): &#39;&#39;&#39; Function for completing a forward pass of the discriminator: Given an image tensor, returns a 1-dimension tensor representing fake/real. Parameters: image: a flattened image tensor with dimension (im_dim) &#39;&#39;&#39; return self.disc(image) . Training . First, we will set your parameters: . criterion: the loss function | n_epochs: the number of times you iterate through the entire dataset when training | z_dim: the dimension of the noise vector | display_step: how often to display/visualize the images | batch_size: the number of images per forward/backward pass | lr: the learning rate | device: the device type, here using a GPU (which runs CUDA), not CPU | . Next, we will load the MNIST dataset as tensors using a dataloader. . criterion = nn.BCEWithLogitsLoss() n_epochs = 500 z_dim = 64 display_step = 10000 batch_size = 64 lr = 0.00001 # Load MNIST dataset as tensors dataloader = DataLoader( MNIST(&#39;.&#39;, download=True, transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True) ### Set the device for training device = &#39;cuda&#39; . Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw . Now, we can initialize the generator, discriminator, and optimizers. Note that each optimizer only takes the parameters of one particular model, since we want each optimizer to optimize only one of the models. . gen = Generator(z_dim).to(device) gen_opt = torch.optim.Adam(gen.parameters(), lr=lr) disc = Discriminator().to(device) disc_opt = torch.optim.Adam(disc.parameters(), lr=lr) . Before we train our GAN network, we will need to create functions to calculate the discriminator&#39;s loss and the generator&#39;s loss. This is how the discriminator and generator will know how they are doing and improve themselves. . def get_disc_loss(gen, disc, criterion, real, num_images, z_dim, device): &#39;&#39;&#39; Return the loss of the discriminator given inputs. Parameters: gen: the generator model, which returns an image given z-dimensional noise disc: the discriminator model, which returns a single-dimensional prediction of real/fake criterion: the loss function, which should be used to compare the discriminator&#39;s predictions to the ground truth reality of the images (e.g. fake = 0, real = 1) real: a batch of real images num_images: the number of images the generator should produce, which is also the length of the real images z_dim: the dimension of the noise vector, a scalar device: the device type Returns: disc_loss: a torch scalar loss value for the current batch &#39;&#39;&#39; # Create noise vectors and generate a batch (num_images) of fake images. z_noise = get_noise(num_images, z_dim, device) fake_imgs = gen(z_noise) # Get the discriminator&#39;s prediction of the fake image and calculate the loss disc_fake_pred = disc(fake_imgs.detach()) disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred)) # Get the discriminator&#39;s prediction of the real image and calculate the loss. disc_real_pred = disc(real) disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred)) # Calculate the discriminator&#39;s loss by averaging the real and fake loss disc_loss = (disc_real_loss + disc_fake_loss)/2 return disc_loss . def get_gen_loss(gen, disc, criterion, num_images, z_dim, device): &#39;&#39;&#39; Return the loss of the generator given inputs. Parameters: gen: the generator model, which returns an image given z-dimensional noise disc: the discriminator model, which returns a single-dimensional prediction of real/fake criterion: the loss function, which should be used to compare the discriminator&#39;s predictions to the ground truth reality of the images (e.g. fake = 0, real = 1) num_images: the number of images the generator should produce, which is also the length of the real images z_dim: the dimension of the noise vector, a scalar device: the device type Returns: gen_loss: a torch scalar loss value for the current batch &#39;&#39;&#39; # Create noise vectors and generate a batch of fake images. z_noise = get_noise(num_images, z_dim, device) fake_imgs = gen(z_noise) # Get the discriminator&#39;s prediction of the fake image. disc_fake_pred = disc(fake_imgs) # Calculate the generator&#39;s loss (Note: It compares the generator output of fake images with the real ones) gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) return gen_loss . cur_step = 0 mean_generator_loss = 0 mean_discriminator_loss = 0 test_generator = True # Whether the generator should be tested gen_loss = False error = False for epoch in range(n_epochs): # Dataloader returns the batches for real, _ in tqdm(dataloader): cur_batch_size = len(real) # Flatten the batch of real images from the dataset real = real.view(cur_batch_size, -1).to(device) ### Update discriminator ### # Zero out the gradients before backpropagation disc_opt.zero_grad() # Calculate discriminator loss disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device) # Update gradients disc_loss.backward(retain_graph=True) # Update optimizer disc_opt.step() # For testing purposes, to keep track of the generator weights if test_generator: old_generator_weights = gen.gen[0][0].weight.detach().clone() gen_opt.zero_grad() gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device) gen_loss.backward() gen_opt.step() # For testing purposes, to check that your code changes the generator weights if test_generator: try: assert lr &gt; 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() &lt; 0.0005 and epoch == 0) assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights) except: error = True print(&quot;Runtime tests have failed&quot;) # Keep track of the average discriminator loss mean_discriminator_loss += disc_loss.item() / display_step # Keep track of the average generator loss mean_generator_loss += gen_loss.item() / display_step ### Visualization code ### if cur_step % display_step == 0 and cur_step &gt; 0: print(f&quot;Epoch {epoch}, step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}&quot;) fake_noise = get_noise(cur_batch_size, z_dim, device=device) fake = gen(fake_noise) show_tensor_images(fake) show_tensor_images(real) mean_generator_loss = 0 mean_discriminator_loss = 0 cur_step += 1 . Epoch 10, step 10000: Generator loss: 5.4047161012649445, discriminator loss: 0.05443974889267115 . Epoch 21, step 20000: Generator loss: 6.291417294359194, discriminator loss: 0.05477201426360978 . Epoch 31, step 30000: Generator loss: 3.850531491291531, discriminator loss: 0.15501205386193492 . Epoch 42, step 40000: Generator loss: 2.3283984214425137, discriminator loss: 0.23069154401868655 . Epoch 53, step 50000: Generator loss: 2.0571084298729923, discriminator loss: 0.2636471902921786 . Epoch 63, step 60000: Generator loss: 1.6916157222747796, discriminator loss: 0.3298123072624213 . Epoch 74, step 70000: Generator loss: 1.479275153839581, discriminator loss: 0.3825816682457922 . Epoch 85, step 80000: Generator loss: 1.3958063300907644, discriminator loss: 0.4082342538863425 . Epoch 95, step 90000: Generator loss: 1.4873849587976908, discriminator loss: 0.38783631078004704 . Epoch 106, step 100000: Generator loss: 1.5395997340500345, discriminator loss: 0.3742863032758239 . Epoch 117, step 110000: Generator loss: 1.500580434679985, discriminator loss: 0.3821151517182578 . Epoch 127, step 120000: Generator loss: 1.3551663604080733, discriminator loss: 0.4202767654120942 . Epoch 138, step 130000: Generator loss: 1.3603083747446494, discriminator loss: 0.41840620750039575 . Epoch 149, step 140000: Generator loss: 1.3575507786154786, discriminator loss: 0.42234055254906416 . Epoch 159, step 150000: Generator loss: 1.4248633385539051, discriminator loss: 0.40728020892888367 . Epoch 170, step 160000: Generator loss: 1.47160139136314, discriminator loss: 0.394411649782956 . Epoch 181, step 170000: Generator loss: 1.4830472575783757, discriminator loss: 0.3922100943788883 . Epoch 191, step 180000: Generator loss: 1.4683524356603588, discriminator loss: 0.40119695761352636 . Epoch 202, step 190000: Generator loss: 1.4103956043541401, discriminator loss: 0.4197945091888331 . Epoch 213, step 200000: Generator loss: 1.3819597438156572, discriminator loss: 0.423482037006321 . Epoch 223, step 210000: Generator loss: 1.4246015510618661, discriminator loss: 0.41522652971297497 . Epoch 234, step 220000: Generator loss: 1.4182038937389847, discriminator loss: 0.41454712654650205 . Epoch 245, step 230000: Generator loss: 1.4623772296845878, discriminator loss: 0.40369796157926335 . Epoch 255, step 240000: Generator loss: 1.4457483284533021, discriminator loss: 0.4046686042502515 . Epoch 266, step 250000: Generator loss: 1.38512691426873, discriminator loss: 0.4234224717453112 . Epoch 277, step 260000: Generator loss: 1.352188807457682, discriminator loss: 0.4336424077853562 . Epoch 287, step 270000: Generator loss: 1.3844688882827767, discriminator loss: 0.41897474683076086 . Epoch 298, step 280000: Generator loss: 1.3627459356606006, discriminator loss: 0.4291179351970558 . Epoch 309, step 290000: Generator loss: 1.3106346433222356, discriminator loss: 0.44792365804612694 . Epoch 319, step 300000: Generator loss: 1.3448638754308249, discriminator loss: 0.4394607469886559 . Epoch 330, step 310000: Generator loss: 1.3012820241808845, discriminator loss: 0.4524194141492234 . Epoch 341, step 320000: Generator loss: 1.344684604495768, discriminator loss: 0.4374074421599516 . Epoch 351, step 330000: Generator loss: 1.2217702299892876, discriminator loss: 0.47974520444720825 . Epoch 362, step 340000: Generator loss: 1.231750753808027, discriminator loss: 0.4726514756649734 . Epoch 373, step 350000: Generator loss: 1.2394801887631413, discriminator loss: 0.4716021126791837 . Epoch 383, step 360000: Generator loss: 1.2483579675018717, discriminator loss: 0.4689559111356739 . Epoch 394, step 370000: Generator loss: 1.2837351714789904, discriminator loss: 0.4569161177024241 . Epoch 405, step 380000: Generator loss: 1.294854753339291, discriminator loss: 0.4585107137933375 . Epoch 415, step 390000: Generator loss: 1.2179591452300491, discriminator loss: 0.47934117802828724 . Epoch 426, step 400000: Generator loss: 1.2247088456273103, discriminator loss: 0.4760446860566737 . Epoch 437, step 410000: Generator loss: 1.1542475645661356, discriminator loss: 0.5038219578146936 . Epoch 447, step 420000: Generator loss: 1.0992902897894385, discriminator loss: 0.5215150975674391 . Epoch 458, step 430000: Generator loss: 1.0931267674624914, discriminator loss: 0.5264235710889109 . Epoch 469, step 440000: Generator loss: 1.1034114745616896, discriminator loss: 0.5200620216712355 . Epoch 479, step 450000: Generator loss: 1.0829909142255831, discriminator loss: 0.5259332243412744 . Epoch 490, step 460000: Generator loss: 1.0734401089608672, discriminator loss: 0.5309209023088215 .",
            "url": "https://aaabulkhair.github.io/Analixa/gan/deep%20learning/pytorch/python/2022/03/18/gan-generating-hand-written-digits-pytorch-1.html",
            "relUrl": "/gan/deep%20learning/pytorch/python/2022/03/18/gan-generating-hand-written-digits-pytorch-1.html",
            "date": " • Mar 18, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "GPT-3, The Model Simply Knows!",
            "content": ". Natural Language Processing (NLP) has been one of the most challenging areas in deep learning. This is due to several reasons. First, human language is complicated, even for humans themselves! . Consider asking someone about his experience in learning Chinese, for example. Without a doubt, he will tell you that this is difficult. The difficulty in learning any language is that almost all the meaning can be derived from the contextual conversational pipeline. In other words, you can’t tell what the purpose. Let’s see the following example: . The UN sanctioned Iran’s use of nuclear power (allowed) | The UN agreed on sanctions for Iran because they used nuclear power (punishment) This example is callous even for a native English speaker, and it would be tougher for a model to make some kind of reasoning for these two sentences. | . Second, language is handled as sequences that can vary in input or output length, which is another obstacle for the modeling process. Because the model simply does not know what is the most important word in a sequence that could predict the next works in a conversational pipeline. . Now, let’s see some of the revolutionary approaches in NLP that made a real leap! . Attention is All You Need . This paper was a turning point in all NLP. It solves most of the context problems. That means we can finally solve the problem of the most essential or related past words in predicting the next ones using a structured mathematical description. . . This paper laid the foundation for the use of Transformers (with attention heads) instead of Sequence Models (RNNs) only. . Afterward, these concepts paved the way towards a new breed of language models that are capable of reasoning in some sense. . BERT . BERT which stands for Bidirectional Encoder Representations from Transformers found in this paper put the idea of transformers with attention heads at the top of language models. BERT was, breathtakingly, a model that marks a new era in language models. The idea behind BERT is simple as it constructs out of two main stages. . Pre-training, in which we just train our model on a large corpus to do some supervised tasks like next sentence prediction ( NSP ). | Fine Tuning, another training step using a relatively smaller dataset to a more specialized task. | . . BERT, with its 345 million parameters, was able to achieve very superior performance over any model of its kind. . GPT-3 . A gigantic model, the largest model ever built by humans. GPT-3, presented by OpenAI in this paper, is the state-of-the-art (SOTA) language model with a very superior model that is capable of doing the most NLP-related tasks. . Now, we will talk about different aspects of the model from the training data, the number of parameters, and the training procedures. . Training Datasets . GPT-3 is trained on almost the entire internet or, according to my expression, trained on the whole human civilization. . The corpus is the largest ever collected for any language model ever. It’s kind of mind-blowing to be thinking about how enormous it is! The following table from the original paper depicts the datasets and their weight in the training mix. . . These datasets would sum up to 500 billion tokens, which is incredibly massive and incomparable to any other language model. . In a nutshell, these datasets are forming the whole human knowledge! Almost every webpage you have seen in your life is included in this training process! Let’s move on to the training procedure. . Training . We are waiting for OpenAI to reveal more details about the training infrastructure and model implementation. But to put things into perspective, the GPT-3 175B model required 3.14E23 FLOPS of computing for training. Even at theoretical 28 TFLOPS for V100 and lowest three years reserved cloud pricing we could find, this will take 355 GPU-years and cost $4.6M for a single training run. Similarly, a single RTX 8000, assuming 15 TFLOPS, would take 665 years to run. . Time is not the only enemy. The 175 Billion parameters need 700GB memory to store. This is one order of magnitude larger than the maximum memory in a single GPU (48 GB of Quadro RTX 8000). To train the larger models without running out of memory, the OpenAI team uses a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU’s on the part of a high-bandwidth cluster provided by Microsoft. . The following graph is showing some information about the training power required to train different sizes of language models in Petaflop/s-days. . . Model Architecture . GPT-3 comes in eight sizes, ranging from 125M to 175B parameters. The largest GPT-3 model is an order of magnitude larger than the previous record-holder, T5-11B. The smallest GPT-3 model is roughly the size of BERT-Base and RoBERTa-Base. . All GPT-3 models use the same attention-based architecture as their GPT-2 predecessor. The smallest GPT-3 model (125M) has 12 attention layers, each with 12x 64-dimension heads. The largest GPT-3 model (175B) uses 96 attention layers, each with 96x 128-dimension heads. . GPT-3 expanded the capacity of its GPT-2 by three orders of magnitudes without significant modification of the model architecture — just more layers, wider layers, and more data to train it on. . Learning Philosophy . Unlike all previous models that are trained on next word prediction and then be fine-tuned on a specific task, GPT-3 is only trained on next word prediction only with no other fine-tuning on any task. However, GPT-3 is doing surprisingly well! Let’s dive into more details about this! . . As the comparison shows, the model is merely doing almost any task just by remembering it. Some tasks can be achieved and in an excellent performance with no remembering at all. These are zero-shot learning tasks. Others will require one example to follow, and those can be called one-shot learning tasks. Besides, the most difficult tasks that may require several cases to remember are called few-shot learning tasks. . Despite that this approach is straightforward, it’s useful! I think that every natural language task in the human brain is handled that way. For example, programmers, after spending some time in programming they can immediately think of code as a text completion in their minds, and that’s all. . The effect of this simple approach did not prove itself before. Only when having the required training data and the enormous model, it become crystal clear! . Applications . This may be the most breathtaking part about GPT-3. In the following passages, I will try to view most of these applications. . Text Generation . This is GPT’s rockstar application – a conditional generative model that creates near-human level quality text content. Given the beginning of some articles, the model is asked to generate the rest of the story in a word-by-word fashion. . More precisely, GPT-3 is presented with a title, a subtitle, and the prompt word “Article:” It then writes short articles (~200 words) that fools humans most of the time. According to OpenAI’s user study, “mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above change at ~52%”. Meaning humans will make random guesses while asking to detect GPT-3 generated articles. In contrast, the mean human accuracy at detecting particles produced by the smallest GPT-3 model (125M) is 76%. . This can be a big deal — “simply” increasing the size of the model by three orders of magnitude can change something that is half-working into something non-distinguishable from human work. In plain English, this empirically shows that the number of model parameters, the FLOP/s-days, and the number of training examples needs to grow according to a power function of the improvement of the model. . Of course, GPT-3 may still produce non-factual content (such as suggesting the popular U.S. TV program “The Tonight Show” is hosted by Megyn Kelly instead of Jimmy Fallon), nor did OpenAI claim the model is ready for writing the last two books of “A Song of Ice and Fire.” Nonetheless, getting closer to the finishing line of the Turing test for writing short articles is significant, and will no doubts have an enormous impact on our social media. . General NLP Tasks . Although writing a new article is excellent, the killer feature of GPT-3 is the ability to be ‘re-programmed’ for general NLP tasks without any finetuning. This is where OpenAI’s real ambition lies: having a model to do just about anything by conditioning it with a few examples. . The paper showed a dozen of downstream tasks, ranging from the usual players such as machine translation and question and answer to the unexpected new tasks such as arithmetic computation and one-shot learning of new words. Instead of reiterating the details of each task, the rest of this article will discuss some common patterns across the board. . In the next lines, we will try to cover some of these cool applications. . Explaining Idioms . Just by using a few-shot learning approach, the model is surprisingly perfect in explaining idioms! See the conversation below, generated by GPT-3. . . Mind-blowing search engine. . It’s not like any search engine because you can ask it in plain English back in plain English also with no search results, only the right and exact answer! . . Layout Generator . Imagine writing some English sentences and get back in return, a full functioning front-end app in a blink of an eye! . . The debate of GPT-3 against another GPT-3! . What if you can see an entirely generated debate just be making a GPT-3 model talk with another! It’s incredible. What is more frightening is that these two models after five messages began to the limitations of human beings. . . Finally, and without a doubt, GPT-3 has changed the face of language modeling from now on. This superior performance will open the door to dozens of philosophical and ethical questions. Some people may wonder about some jobs, such as will these models make some jobs disappear like writers, coders, and teachers. Is it ethical to use this model to pass exams?! ( Note that GPT-3 was able to score 57% in the SAT exam ). I think the future will answer all of these questions! . One more thing that made me can’t wait for the next GPT is that it will support interactive podcasts! And this is really beyond imagination! .",
            "url": "https://aaabulkhair.github.io/Analixa/gpt-3/nlp/research%20paper/2022/03/08/GPT-3,-The-Model-Simply-Knows.html",
            "relUrl": "/gpt-3/nlp/research%20paper/2022/03/08/GPT-3,-The-Model-Simply-Knows.html",
            "date": " • Mar 8, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Text Preprocessing | NLP 1",
            "content": "Introduction . When dealing with text data with Machine Learning models. There&#39;s a huge and simple problem! Which is, . Algorithms can not comprehend text, only numbers! . So, is it all about converting text into numbers? Of course, not! The natural language is a very senstive and expressive type of data which means alot of expressions and desires can be hidden in a piece of text an numbers are less likely to demonestrate that! Numbers can reveal very little information when comparing to the natural text. so, it this set of tutorials, we will try to handle most of the preprocessing techniques. Strating from the very basic (like what we will do in this tutorial) ending up with very complex. . Setup . We will be using the Natural Language Toolkit (NLTK) package and SpaCy, open-source Python libraries for natural language processing. NLTK has modules for collecting, handling, and processing Twitter data. . For this tutorial, we will use a Twitter dataset that comes with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. Let us import them now as well as a few other libraries we will be using. . import nltk # Python library for NLP import spacy # Python libray for NLP from nltk.corpus import twitter_samples # sample Twitter dataset from NLTK import matplotlib.pyplot as plt # library for visualization import seaborn as sns # library for visualization import random # pseudo-random number generator . About the Twitter dataset . The sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial. . You can download the dataset in your workspace by doing: . nltk.download(&#39;twitter_samples&#39;) . [nltk_data] Downloading package twitter_samples to [nltk_data] /usr/share/nltk_data... [nltk_data] Package twitter_samples is already up-to-date! . True . We can load the text fields of the positive and negative tweets by using the module&#39;s strings() method like this: . all_positive_tweets = twitter_samples.strings(&#39;positive_tweets.json&#39;) all_negative_tweets = twitter_samples.strings(&#39;negative_tweets.json&#39;) . Next, we&#39;ll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets . print(&#39;Number of positive tweets: &#39;, len(all_positive_tweets)) print(&#39;Number of negative tweets: &#39;, len(all_negative_tweets)) print(&#39; nThe type of all_positive_tweets is: &#39;, type(all_positive_tweets)) print(&#39;The type of a tweet entry is: &#39;, type(all_negative_tweets[0])) . Number of positive tweets: 5000 Number of negative tweets: 5000 The type of all_positive_tweets is: &lt;class &#39;list&#39;&gt; The type of a tweet entry is: &lt;class &#39;str&#39;&gt; . Now, lets construct a pie chart to point out class distrbution. . colors = sns.color_palette(&#39;pastel&#39;)[0:5] #create pie chart plt.figure(figsize=(8,8)) plt.pie([len(all_positive_tweets), len(all_negative_tweets)], labels = [&#39;Positive&#39;, &#39;Negative&#39;], colors = colors, autopct=&#39;%.0f%%&#39;) plt.show() . Looking at raw texts . Before anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we&#39;d like to consider when preprocessing our data. . Below, you will print one random positive and one random negative tweet. We have added a color mark at the beginning of the string to further distinguish the two. (Warning: This is taken from a public dataset of real tweets and a very small portion has explicit content.) . print(&#39; 033[92m&#39; + all_positive_tweets[random.randint(0,5000)]) # print negative in red print(&#39; 033[91m&#39; + all_negative_tweets[random.randint(0,5000)]) . @PuppyShogun mistakes happen man, as long as we get to play the game, we&#39;ll be happy :) ＠maverickgamer_　July 24, 2015 at 07:17PM 　:( . One observation, is the presence of emoticons and URLs in many of the tweets. This info will come in handy in the next steps. . Steps of Preprocessing . Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks: . Removing unnesseary parts (mentions, tags, hashtags, and URLs) | Lowercasing | Removing stop words and punctuation | Tokenizing the string | Stemming and/or lemmetization | . Besides, the first purpose of preprocessing which is converting text into numbers, there&#39;s one more important purpose, which is about reducing the redundency of the data size itself. In other words, we want to represent the text features in the most minimal feature space and this will be demonestrated clearly for each step. . Let&#39;s import some libraries to help us out! . import re # library for regular expression operations import string # for string operations import spacy # Text processing . tweet = all_positive_tweets[2277] print(tweet) . My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i . Removing unnesseary parts (mentions, tags, hashtags, and URLs) . Since we have a Twitter dataset, we&#39;d like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We&#39;ll use the re library to perform regular expression operations on our tweet. We&#39;ll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. &#39;&#39;) . RT_remover = lambda x : re.sub(r&#39;^b s([RT]+)?&#39;,&#39;&#39;, x) # remove all URLs URL_remover = lambda x: re.sub(r&#39;http S+&#39;, &#39;&#39;, x) # remove hashtags # only removing the hash # sign from the word Hashtag_remover = lambda x: re.sub(r&#39;#&#39;, &#39;&#39;, x) # Apply all functions tweet = RT_remover(tweet) tweet = URL_remover(tweet) tweet = Hashtag_remover(tweet) # print final output print(tweet) . My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… . If you need any further info about using and testing regex, you may visit this website . Lowercasing . This step is often done for the sake of reducing the feature representation space. . For example: . I love cars! Cars are the most passionate hoppy in the world. | . Without lowercasing, cars and Cars with be two different words with two different representation which is not true nor optimal for sure. . lowercase = lambda x : x.lower() # Apply function tweet = lowercase(tweet) # Print result print(tweet) . my beautiful sunflowers on a sunny friday morning off :) sunflowers favourites happy friday off… . Tokenization . Although this word seems to be complex but it means a very simple process which is dividing the natural text into parts! This will make the process of generating a feature representation much easier. . Here, we will divide our natural text into words which are the basic building block of a text. We will be doing that using some built-in functions such as split() and strip() . def tokenize(text) -&gt; str: &quot;&quot;&quot; transform text into list of tokens - param: - text: input text -&gt; str - return - text_tokens: list of tokens &quot;&quot;&quot; text = text.strip() text_tokens = text.split() return text_tokens # test the function tweet_tokens = tokenize(tweet) tweet_tokens . [&#39;my&#39;, &#39;beautiful&#39;, &#39;sunflowers&#39;, &#39;on&#39;, &#39;a&#39;, &#39;sunny&#39;, &#39;friday&#39;, &#39;morning&#39;, &#39;off&#39;, &#39;:)&#39;, &#39;sunflowers&#39;, &#39;favourites&#39;, &#39;happy&#39;, &#39;friday&#39;, &#39;off…&#39;] . Removing stop words and punctuation . Stop words and punctuation don&#39;t give a lot of meaning in most cases. Let&#39;s see an example. . There is a ball on the table. It is huge and colorful! . Lets remove stop words and punctuation and see if we can capture the same meaning. . ball table huge colorful . As you can see, it&#39;s much shorter and this will defintly lead to lower feature representation. Some packages have provided lists for the stopwords and punctuation in many languges. In this tutorial, we will be using SpaCy and String packages. . en = spacy.load(&#39;en_core_web_sm&#39;) # Donwload Stopwords stopwords = en.Defaults.stop_words # Convert to list lst_stopwords = list(stopwords) print(&#39;Stop words n&#39;) print(lst_stopwords) print(&#39; nPunctuation n&#39;) print(string.punctuation) . Stop words [&#39;‘s&#39;, &#39;even&#39;, &#39;everything&#39;, &#39;how&#39;, &#39;thereafter&#39;, &#39;being&#39;, &#39;mine&#39;, &#39;up&#39;, &#39;make&#39;, &#39;really&#39;, &#39;’d&#39;, &#39;own&#39;, &#39;becoming&#39;, &#39;none&#39;, &#39;but&#39;, &#39;between&#39;, &#39;down&#39;, &#39;’s&#39;, &#39;whoever&#39;, &#39;been&#39;, &#39;every&#39;, &#39;else&#39;, &#39;’m&#39;, &#39;unless&#39;, &#39;serious&#39;, &#39;ever&#39;, &#39;those&#39;, &#39;above&#39;, &#39;wherever&#39;, &#39;cannot&#39;, &#39;any&#39;, &#39;back&#39;, &#39;here&#39;, &#39;whom&#39;, &#39;to&#39;, &#39;well&#39;, &#39;fifteen&#39;, &#39;each&#39;, &#39;due&#39;, &#39;eight&#39;, &#39;whenever&#39;, &#39;often&#39;, &#39;last&#39;, &#39;both&#39;, &#39;ca&#39;, &#39;throughout&#39;, &#39;go&#39;, &#39;mostly&#39;, &#39;three&#39;, &#39;before&#39;, &#39;or&#39;, &#39;few&#39;, &#39;for&#39;, &#39;such&#39;, &#39;’re&#39;, &#39;would&#39;, &#39;done&#39;, &#39;thru&#39;, &#39;almost&#39;, &#39;eleven&#39;, &#39;other&#39;, &#39;do&#39;, &#39;this&#39;, &#39;hereby&#39;, &#39;along&#39;, &#39;n‘t&#39;, &#34;&#39;d&#34;, &#39;next&#39;, &#39;one&#39;, &#39;seeming&#39;, &#39;were&#39;, &#39;then&#39;, &#39;yourself&#39;, &#39;part&#39;, &#39;wherein&#39;, &#39;while&#39;, &#39;anywhere&#39;, &#39;twenty&#39;, &#39;her&#39;, &#39;beforehand&#39;, &#39;put&#39;, &#39;somehow&#39;, &#39;off&#39;, &#39;either&#39;, &#39;very&#39;, &#39;us&#39;, &#39;perhaps&#39;, &#39;over&#39;, &#39;various&#39;, &#39;no&#39;, &#39;also&#39;, &#39;thus&#39;, &#39;via&#39;, &#39;ours&#39;, &#34;&#39;ll&#34;, &#39;someone&#39;, &#39;onto&#39;, &#39;ten&#39;, &#39;just&#39;, &#39;further&#39;, &#39;had&#39;, &#39;within&#39;, &#39;a&#39;, &#39;nine&#39;, &#39;him&#39;, &#39;his&#39;, &#39;never&#39;, &#39;third&#39;, &#39;full&#39;, &#39;nothing&#39;, &#39;hundred&#39;, &#39;there&#39;, &#39;hers&#39;, &#39;around&#39;, &#39;indeed&#39;, &#39;now&#39;, &#39;can&#39;, &#39;nevertheless&#39;, &#39;‘re&#39;, &#39;therein&#39;, &#39;seem&#39;, &#39;twelve&#39;, &#39;thereby&#39;, &#39;might&#39;, &#39;from&#39;, &#39;others&#39;, &#39;hereupon&#39;, &#39;an&#39;, &#39;again&#39;, &#39;made&#39;, &#39;once&#39;, &#39;though&#39;, &#39;five&#39;, &#39;by&#39;, &#39;‘ll&#39;, &#39;at&#39;, &#39;several&#39;, &#39;whither&#39;, &#39;their&#39;, &#39;our&#39;, &#39;least&#39;, &#39;some&#39;, &#39;too&#39;, &#39;the&#39;, &#39;they&#39;, &#39;except&#39;, &#39;who&#39;, &#39;thereupon&#39;, &#39;meanwhile&#39;, &#39;side&#39;, &#39;you&#39;, &#39;another&#39;, &#39;elsewhere&#39;, &#39;first&#39;, &#39;per&#39;, &#39;yet&#39;, &#39;formerly&#39;, &#39;‘m&#39;, &#39;whence&#39;, &#39;latterly&#39;, &#39;whether&#39;, &#39;used&#39;, &#39;across&#39;, &#39;he&#39;, &#39;enough&#39;, &#39;will&#39;, &#39;whole&#39;, &#34;n&#39;t&#34;, &#39;all&#39;, &#39;we&#39;, &#39;fifty&#39;, &#39;always&#39;, &#39;after&#39;, &#39;i&#39;, &#39;bottom&#39;, &#39;himself&#39;, &#39;n’t&#39;, &#39;afterwards&#39;, &#39;upon&#39;, &#39;with&#39;, &#39;ourselves&#39;, &#39;my&#39;, &#39;therefore&#39;, &#34;&#39;s&#34;, &#39;most&#39;, &#39;top&#39;, &#39;under&#39;, &#39;your&#39;, &#39;not&#39;, &#39;nowhere&#39;, &#39;anyway&#39;, &#39;whatever&#39;, &#39;anyone&#39;, &#39;‘ve&#39;, &#39;through&#39;, &#39;rather&#39;, &#39;whereafter&#39;, &#39;seemed&#39;, &#39;so&#39;, &#39;get&#39;, &#39;nor&#39;, &#39;against&#39;, &#39;moreover&#39;, &#39;yourselves&#39;, &#39;anyhow&#39;, &#39;about&#39;, &#39;move&#39;, &#39;was&#39;, &#39;yours&#39;, &#39;because&#39;, &#39;below&#39;, &#39;six&#39;, &#39;latter&#39;, &#39;everyone&#39;, &#39;noone&#39;, &#39;still&#39;, &#39;does&#39;, &#39;must&#39;, &#39;them&#39;, &#39;call&#39;, &#39;why&#39;, &#39;‘d&#39;, &#39;since&#39;, &#39;amount&#39;, &#39;alone&#39;, &#39;regarding&#39;, &#39;became&#39;, &#39;and&#39;, &#39;may&#39;, &#39;former&#39;, &#39;myself&#39;, &#39;doing&#39;, &#39;less&#39;, &#39;she&#39;, &#39;into&#39;, &#39;’ll&#39;, &#39;sometime&#39;, &#39;although&#39;, &#39;name&#39;, &#39;itself&#39;, &#39;herself&#39;, &#39;besides&#39;, &#39;it&#39;, &#39;as&#39;, &#39;when&#39;, &#39;thence&#39;, &#39;if&#39;, &#39;front&#39;, &#39;seems&#39;, &#39;whereas&#39;, &#39;give&#39;, &#39;these&#39;, &#39;empty&#39;, &#39;themselves&#39;, &#39;many&#39;, &#39;see&#39;, &#39;much&#39;, &#39;become&#39;, &#39;only&#39;, &#39;forty&#39;, &#39;could&#39;, &#39;that&#39;, &#39;what&#39;, &#39;otherwise&#39;, &#39;re&#39;, &#39;on&#39;, &#39;say&#39;, &#39;sometimes&#39;, &#39;amongst&#39;, &#39;in&#39;, &#39;namely&#39;, &#39;together&#39;, &#39;please&#39;, &#39;which&#39;, &#39;towards&#39;, &#39;without&#39;, &#39;hereafter&#39;, &#39;during&#39;, &#39;everywhere&#39;, &#39;has&#39;, &#39;out&#39;, &#39;whose&#39;, &#34;&#39;re&#34;, &#39;more&#39;, &#34;&#39;ve&#34;, &#39;am&#39;, &#39;whereby&#39;, &#39;nobody&#39;, &#39;beside&#39;, &#39;somewhere&#39;, &#39;quite&#39;, &#39;toward&#39;, &#39;me&#39;, &#39;’ve&#39;, &#39;herein&#39;, &#39;using&#39;, &#39;beyond&#39;, &#39;have&#39;, &#39;among&#39;, &#39;than&#39;, &#34;&#39;m&#34;, &#39;where&#39;, &#39;did&#39;, &#39;until&#39;, &#39;sixty&#39;, &#39;whereupon&#39;, &#39;are&#39;, &#39;its&#39;, &#39;however&#39;, &#39;becomes&#39;, &#39;behind&#39;, &#39;something&#39;, &#39;neither&#39;, &#39;same&#39;, &#39;already&#39;, &#39;four&#39;, &#39;two&#39;, &#39;is&#39;, &#39;show&#39;, &#39;keep&#39;, &#39;of&#39;, &#39;anything&#39;, &#39;should&#39;, &#39;hence&#39;, &#39;be&#39;, &#39;take&#39;] Punctuation !&#34;#$%&amp;&#39;()*+,-./:;&lt;=&gt;?@[ ]^_`{|}~ . tweets_clean = list() # clean all word tokens for wrd in tweet_tokens: # Iterate over all words if (wrd not in lst_stopwords and wrd not in set(string.punctuation)): tweets_clean.append(wrd) # Add to clean tweets list tweets_clean . [&#39;beautiful&#39;, &#39;sunflowers&#39;, &#39;sunny&#39;, &#39;friday&#39;, &#39;morning&#39;, &#39;:)&#39;, &#39;sunflowers&#39;, &#39;favourites&#39;, &#39;happy&#39;, &#39;friday&#39;, &#39;off…&#39;] . Stemming and/or lemmetization . Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary. . Consider the words: . learn | learning | learned | learnt | . All these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, happi and sunni. That&#39;s because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy: . happy | happiness | happier | . We can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen. . Lemmetization is the process of coverting a word into the least meaningfull part of it. Unlike stemming, lemmetization has to produce the least correct root with correct spelling. . Word Lemma . happiness | happiness | . happier | happy | . happier | happy | . Now, lets see some words that might make you feel the differnce. . Word Stem Lemma . caring | care | caring | . ties | ti | tie | . easily | easili | easily | . mice | mice | mouse | . As you can see, stemming can sometimes produce uncorrect words such as ti and easili. On contrary, the lemmetization always produce a correct word in spelling even if this will lead to higher feature representation. For example, caring and care will be to different elements in the feature representation, however they share the same root car when dealing with stemming instead. We will be using NLTK library for that purpose for lemmetizing this data. . from nltk.stem import WordNetLemmatizer # Intiate WordNetLemmatizer lemmatizer = WordNetLemmatizer() # Lemmetize the words tweets_lemmas = [] for wrd in tweets_clean: wrd = lemmatizer.lemmatize(wrd) tweets_lemmas.append(wrd) tweets_lemmas . [&#39;beautiful&#39;, &#39;sunflower&#39;, &#39;sunny&#39;, &#39;friday&#39;, &#39;morning&#39;, &#39;:)&#39;, &#39;sunflower&#39;, &#39;favourite&#39;, &#39;happy&#39;, &#39;friday&#39;, &#39;off…&#39;] . Putting it all together. . By now, we have completed all the required preprocessing steps. Lets combine them all in a one preprocess function that will be used in our next tutorial. . def preprocess(txt) -&gt; str: # remove old style retweet text &quot;RT&quot; RT_remover = lambda x : re.sub(r&#39;^b s([RT]+)?&#39;,&#39;&#39;, x) # remove all URLs URL_remover = lambda x: re.sub(r&#39;http S+&#39;, &#39;&#39;, x) # remove hashtags # only removing the hash # sign from the word Hashtag_remover = lambda x: re.sub(r&#39;#&#39;, &#39;&#39;, x) # Apply all functions txt = RT_remover(txt) txt = URL_remover(txt) txt = Hashtag_remover(txt) def tokenize(text) -&gt; str: &quot;&quot;&quot; transform text into list of tokens - param: - text: input text -&gt; str - return - text_tokens: list of tokens &quot;&quot;&quot; text = text.strip() text_tokens = text.split() return text_tokens lst_txt = tokenize(txt) # Import modules import string import spacy from nltk.stem import WordNetLemmatizer # Load the core utils for english languge en = spacy.load(&#39;en_core_web_sm&#39;) # Donwload Stopwords stopwords = en.Defaults.stop_words # Convert to list lst_stopwords = list(stopwords) lst_txt_clean = list() # clean all word tokens for wrd in lst_txt: # Iterate over all words if (wrd not in lst_stopwords and wrd not in set(string.punctuation)): lst_txt_clean.append(wrd) # Add to clean tweets list lemmatizer = WordNetLemmatizer() # Lemmetize the words lst_txt_lemmas = [] for wrd in lst_txt_clean: wrd = lemmatizer.lemmatize(wrd) lst_txt_lemmas.append(wrd) return lst_txt_lemmas . Let&#39;s test it! . sample_tweet = all_positive_tweets[random.randint(0,5000)] preprocess(sample_tweet) . [&#39;Well&#39;, &#39;morning&#39;, &#39;carry&#39;, &#39;great.&#39;, &#39;Work&#39;, &#39;:)&#39;] . That is the end of this tutorial! If you find this beneficial. Follow me for more updates! . LinkedIn | Kaggle | .",
            "url": "https://aaabulkhair.github.io/Analixa/machine%20learning/nlp/nltk/2022/02/07/nlp-text-preprocessing-1.html",
            "relUrl": "/machine%20learning/nlp/nltk/2022/02/07/nlp-text-preprocessing-1.html",
            "date": " • Feb 7, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Titanic | The Power of Sklearn",
            "content": "Introduction . Neither Titanic dataset nor sklearn a new thing for any data scientist but there are some important features in scikit-learn that will make any model preprocessing and tuning easier, to be specific this notebook will cover the following concepts: . ColumnTransformer | Pipeline | SimpleImputer | StandardScalar | OneHotEncoder | OrdinalEncoder | GridSearch | . Note, this tutorial is a solution to the famous kaggle competition Titanic - Machine Learning from Disaster . Mounting Filesystem . import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # Any results you write to the current directory are saved as output. . /kaggle/input/titanic/gender_submission.csv /kaggle/input/titanic/test.csv /kaggle/input/titanic/train.csv . Import Packages . import pandas as pd # Numpy for Numerical operations import numpy as np # Import ColumnTransformer from sklearn.compose import ColumnTransformer # Import Pipeline from sklearn.pipeline import Pipeline # Import SimpleImputer from sklearn.impute import SimpleImputer # Import StandardScaler, OneHotEncodr and OrdinalEncoder from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder # Import Random Forest for Classification from sklearn.ensemble import RandomForestClassifier # Import GridSearch from sklearn.model_selection import GridSearchCV . Reading Data . In the following cells, we will read the train and test data and check for NaNs. . train_data = pd.read_csv(&quot;/kaggle/input/titanic/train.csv&quot;) # See some info train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): PassengerId 891 non-null int64 Survived 891 non-null int64 Pclass 891 non-null int64 Name 891 non-null object Sex 891 non-null object Age 714 non-null float64 SibSp 891 non-null int64 Parch 891 non-null int64 Ticket 891 non-null object Fare 891 non-null float64 Cabin 204 non-null object Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB . It&#39;s obvious that we had to deal with NaNs . test_data = pd.read_csv(&quot;/kaggle/input/titanic/test.csv&quot;) test_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): PassengerId 418 non-null int64 Pclass 418 non-null int64 Name 418 non-null object Sex 418 non-null object Age 332 non-null float64 SibSp 418 non-null int64 Parch 418 non-null int64 Ticket 418 non-null object Fare 417 non-null float64 Cabin 91 non-null object Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB . Splitting Data . X_train = train_data.drop([&#39;Survived&#39;, &#39;Name&#39;], axis = 1) X_test = test_data.drop([&#39;Name&#39;], axis = 1) y_train = train_data[&#39;Survived&#39;] . Continuous and Numerical features handling . It&#39;s clear that we have some numerical features that have some missing values to be imputed and they have to be of the same scale also. . In the following cell, we will handle the numerical features separtely i.e &quot;Age&quot; and &quot;Fare&quot; . # Difine a list with the numeric features numeric_features = [&#39;Age&#39;, &#39;Fare&#39;] # Define a pipeline for numer&quot;ic features numeric_features_pipeline = Pipeline(steps= [ (&#39;imputer&#39;, SimpleImputer(strategy = &#39;median&#39;)), # Impute with median value for missing (&#39;scaler&#39;, StandardScaler()) # Conduct a scaling step ]) . Categorical features handling . It&#39;s clear that we have some categorical features that have some missing values to be imputed and they have to be encoded using one hot encoding. . In the following cell, we will handle the categorical features separtely i.e &quot;Embarked&quot; and &quot;Sex&quot; . Note: I choose simple imputer for the missing cells to impute with &#39;missing&#39; word. My aim was to gather all missing cells in one category for further encoding. . # Difine a list with the categorical features categorical_features = [&#39;Embarked&#39;, &#39;Sex&#39;] # Define a pipeline for categorical features categorical_features_pipeline = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value = &#39;missing&#39;)), # Impute with the word &#39;missing&#39; for missing values (&#39;onehot&#39;, OneHotEncoder(handle_unknown = &#39;ignore&#39;)) # Convert all categorical variables to one hot encoding ]) . Ordinal features handling . Passenger class or &#39;Pclass&#39; for short is an ordinal feature that must be handled keeping in mind that class 3 is much higher than 2 and so on. . # Define a list with the ordinal features ordinal_features = [&#39;Pclass&#39;] # Define a pipline for ordinal features ordinal_features_pipeline = Pipeline(steps=[ (&#39;ordinal&#39;, OrdinalEncoder(categories= [[1, 2, 3]])) ]) . Construct a comprehended preprocessor . Now, we will create a preprocessor that can handle all columns in our dataset using ColumnTransformer . preprocessor = ColumnTransformer(transformers= [ # transformer with name &#39;num&#39; that will apply # &#39;numeric_features_pipeline&#39; to numeric_features (&#39;num&#39;, numeric_features_pipeline, numeric_features), # transformer with name &#39;cat&#39; that will apply # &#39;categorical_features_pipeline&#39; to categorical_features (&#39;cat&#39;, categorical_features_pipeline, categorical_features), # transformer with name &#39;ord&#39; that will apply # &#39;ordinal_features_pipeline&#39; to ordinal_features (&#39;ord&#39;, ordinal_features_pipeline, ordinal_features) ]) . Prediction Pipeline . Now, we will create a full prediction pipeline that uses our preprocessor and then transfer it to our classifier of choice &#39;Random Forest&#39;. . clf = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;classifier&#39;, RandomForestClassifier(n_estimators = 120, max_leaf_nodes = 100))]) . Pipeline Training . Let&#39;s train our pipeline now . clf.fit(X_train, y_train) . Pipeline(memory=None, steps=[(&#39;preprocessor&#39;, ColumnTransformer(n_jobs=None, remainder=&#39;drop&#39;, sparse_threshold=0.3, transformer_weights=None, transformers=[(&#39;num&#39;, Pipeline(memory=None, steps=[(&#39;imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy=&#39;median&#39;, verbose=0)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean... RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=100, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=120, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False))], verbose=False) . Pipeline Tuning . The question now, can we push it a little bit further? i.e. can we tune every single part or our Pipeline? . Here, I will use GridSearch to decide three things: . Simple Imputer strategy :mean or median&gt; - n_estimators of Random Forest | max leaf nodes of Random Forest | . Note, you can access any parameter from the outer level to the next adjacent inner one . For Example:to access the strategy of the Simple Imputer you can do the followingpreprocessornumimputer__strategy . Let&#39;s see this into action . param_grid = { &#39;preprocessor__num__imputer__strategy&#39;: [&#39;mean&#39;, &#39;median&#39;], &#39;classifier__n_estimators&#39;: [100, 120, 150, 170, 200], &#39;classifier__max_leaf_nodes&#39; : [100, 120, 150, 170, 200] } grid_search = GridSearchCV(clf, param_grid, cv=10) grid_search.fit(X_train, y_train) print((&quot;best random forest from grid search: %.3f&quot; % grid_search.score(X_train, y_train))) print(&#39;The best parameters of Simple Imputer and C are:&#39;) print(grid_search.best_params_) . best random forest from grid search: 0.944 The best parameters of Simple Imputer and C are: {&#39;classifier__max_leaf_nodes&#39;: 100, &#39;classifier__n_estimators&#39;: 150, &#39;preprocessor__num__imputer__strategy&#39;: &#39;median&#39;} . Generate Predictions . Let&#39;s generate predictions now using our grid search model and submit the results . predictions = grid_search.predict(X_test) # Generate results dataframe results_df = pd.DataFrame({&#39;PassengerId&#39;: test_data.PassengerId, &#39;Survived&#39;: predictions}) # Save to csv file results_df.to_csv(&#39;submission.csv&#39;, index = False) print(&#39;Submission CSV has been saved!&#39;) . Submission CSV has been saved! .",
            "url": "https://aaabulkhair.github.io/Analixa/machine%20learning/sklearn/python/2022/02/05/titanic-power-of-scikit-learn.html",
            "relUrl": "/machine%20learning/sklearn/python/2022/02/05/titanic-power-of-scikit-learn.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Choosing Best n_estimators for RandomForest model without retraining",
            "content": "In this notebook, we will try to determine the best number of n_estimators for RandomForest model without training the model for multiple times . Load Dataset . We will use one of the built-in datasets, which is digits . import sklearn.datasets from sklearn.model_selection import train_test_split # Load dataset X, y = sklearn.datasets.load_digits(n_class = 10,return_X_y = True) # Split the data X_train, X_val, y_train, y_val = train_test_split(X, y) . Import libraries . import numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score . Step 1: first fit a Random Forest to the data. Set n_estimators to a high value. . rf = RandomForestClassifier(n_estimators=500, max_depth=4, n_jobs=-1) rf.fit(X_train, y_train) . RandomForestClassifier(max_depth=4, n_estimators=500, n_jobs=-1) . Step 2: Get predictions for each tree in Random Forest separately. . predictions = [] for tree in rf.estimators_: predictions.append(tree.predict_proba(X_val)[None, :]) . Step 3: Concatenate the predictions to a tensor of size (number of trees, number of objects, number of classes). . predictions = np.vstack(predictions) . Step 4: Сompute cumulative average of the predictions. That will be a tensor, that will contain predictions of the random forests for each n_estimators. . cum_mean = np.cumsum(predictions, axis=0)/np.arange(1, predictions.shape[0] + 1)[:, None, None] . Step 5: Get accuracy scores for each n_estimators value . scores = [] for pred in cum_mean: scores.append(accuracy_score(y_val, np.argmax(pred, axis=1))) . That is it! Plot the resulting scores to obtain similar plot to one that appeared on the slides. . plt.figure(figsize=(10, 6)) plt.plot(scores, linewidth=3) plt.xlabel(&#39;num_trees&#39;) plt.ylabel(&#39;accuracy&#39;); . We see, that 150 trees are already sufficient to have stable result. .",
            "url": "https://aaabulkhair.github.io/Analixa/machine%20learning/randomforest/classification/python/2022/02/05/best-n-estimators-for-randomforest.html",
            "relUrl": "/machine%20learning/randomforest/classification/python/2022/02/05/best-n-estimators-for-randomforest.html",
            "date": " • Feb 5, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This blog is powered by Ahmed Abulkhair a Data Scientist and Teaching Assistant at Information Technology Institute, Data Science track, with a strong math background and +3 years of experience in predictive modeling, data processing, machine learning, and deep learning. Also, I have a very special interest to NLP, and GANs! .",
          "url": "https://aaabulkhair.github.io/Analixa/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://aaabulkhair.github.io/Analixa/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}